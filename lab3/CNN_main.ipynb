{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6376419e",
   "metadata": {},
   "source": [
    "# 实验任务二：使用CNN来进行图像分类\n",
    "## CIFAR-10 数据集\n",
    "本次实验使用CIFAR-10 数据集来进行实验。\n",
    "CIFAR-10 数据集包含 60,000 张 32×32 像素的彩色图像，\n",
    "分为 10 个类别，每个类别有 6,000 张图像。\n",
    "具体类别包括飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。\n",
    "数据集被分为训练集和测试集，\n",
    "其中训练集包含 50,000 张图像，测试集包含 10,000 张图像。\n",
    "## 1. 在CIFAR数据集上实现CNN\n",
    "本次任务要求补全代码中空缺部分，包括实现一个CNN类，以及训练过程代码\n",
    "\n",
    "数据集下载链接：\n",
    "\n",
    "https://box.nju.edu.cn/f/d59d5d910d754c3091f5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7434d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81602cb6",
   "metadata": {},
   "source": [
    "导入CIFAR-10数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd51464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 下载并加载训练集\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 创建数据加载器\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c307b5",
   "metadata": {},
   "source": [
    "定义CNN网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c80fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        #TODO: 实现模型结构\n",
    "        #TODO 实现self.conv1:卷积层\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        #TODO 实现self.conv2:卷积层\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        #TODO 实现self.pool: MaxPool2d\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #TODO 实现self.fc1: 线性层\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128) \n",
    "        #TODO 实现self.fc2：线性层\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        #TODO 实现 self.dropout: Dropout层\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555e410",
   "metadata": {},
   "source": [
    "进行训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47923b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, device):\n",
    "    num_epochs = 15\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            #TODO:实现训练部分，完成反向传播过程\n",
    "            #TODO: optimizer梯度清除\n",
    "            optimizer.zero_grad()\n",
    "            #TODO: 模型输入\n",
    "            outputs = model(inputs)\n",
    "            #TODO: 计算损失\n",
    "            loss = criterion(outputs, labels)\n",
    "            #TODO: 反向传播\n",
    "            loss.backward()\n",
    "            #TODO: 更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # 每100个batch打印一次损失\n",
    "                print(\n",
    "                    f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # 每个epoch结束后在测试集上评估模型\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e70fb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [100/1563], Loss: 2.1076\n",
      "Epoch [1/15], Step [200/1563], Loss: 1.8159\n",
      "Epoch [1/15], Step [300/1563], Loss: 1.6787\n",
      "Epoch [1/15], Step [400/1563], Loss: 1.5969\n",
      "Epoch [1/15], Step [500/1563], Loss: 1.5564\n",
      "Epoch [1/15], Step [600/1563], Loss: 1.5274\n",
      "Epoch [1/15], Step [700/1563], Loss: 1.4809\n",
      "Epoch [1/15], Step [800/1563], Loss: 1.4616\n",
      "Epoch [1/15], Step [900/1563], Loss: 1.4611\n",
      "Epoch [1/15], Step [1000/1563], Loss: 1.3980\n",
      "Epoch [1/15], Step [1100/1563], Loss: 1.3840\n",
      "Epoch [1/15], Step [1200/1563], Loss: 1.3715\n",
      "Epoch [1/15], Step [1300/1563], Loss: 1.3808\n",
      "Epoch [1/15], Step [1400/1563], Loss: 1.3397\n",
      "Epoch [1/15], Step [1500/1563], Loss: 1.3598\n",
      "Test Accuracy: 58.93%\n",
      "Epoch [2/15], Step [100/1563], Loss: 1.2549\n",
      "Epoch [2/15], Step [200/1563], Loss: 1.2481\n",
      "Epoch [2/15], Step [300/1563], Loss: 1.2726\n",
      "Epoch [2/15], Step [400/1563], Loss: 1.2315\n",
      "Epoch [2/15], Step [500/1563], Loss: 1.2538\n",
      "Epoch [2/15], Step [600/1563], Loss: 1.2251\n",
      "Epoch [2/15], Step [700/1563], Loss: 1.1786\n",
      "Epoch [2/15], Step [800/1563], Loss: 1.2156\n",
      "Epoch [2/15], Step [900/1563], Loss: 1.1469\n",
      "Epoch [2/15], Step [1000/1563], Loss: 1.1909\n",
      "Epoch [2/15], Step [1100/1563], Loss: 1.1911\n",
      "Epoch [2/15], Step [1200/1563], Loss: 1.1841\n",
      "Epoch [2/15], Step [1300/1563], Loss: 1.1795\n",
      "Epoch [2/15], Step [1400/1563], Loss: 1.1676\n",
      "Epoch [2/15], Step [1500/1563], Loss: 1.1356\n",
      "Test Accuracy: 63.77%\n",
      "Epoch [3/15], Step [100/1563], Loss: 1.0830\n",
      "Epoch [3/15], Step [200/1563], Loss: 1.0853\n",
      "Epoch [3/15], Step [300/1563], Loss: 1.1069\n",
      "Epoch [3/15], Step [400/1563], Loss: 1.1340\n",
      "Epoch [3/15], Step [500/1563], Loss: 1.0846\n",
      "Epoch [3/15], Step [600/1563], Loss: 1.0723\n",
      "Epoch [3/15], Step [700/1563], Loss: 1.0778\n",
      "Epoch [3/15], Step [800/1563], Loss: 1.0912\n",
      "Epoch [3/15], Step [900/1563], Loss: 1.0496\n",
      "Epoch [3/15], Step [1000/1563], Loss: 1.0655\n",
      "Epoch [3/15], Step [1100/1563], Loss: 1.0586\n",
      "Epoch [3/15], Step [1200/1563], Loss: 1.0549\n",
      "Epoch [3/15], Step [1300/1563], Loss: 1.0631\n",
      "Epoch [3/15], Step [1400/1563], Loss: 1.0826\n",
      "Epoch [3/15], Step [1500/1563], Loss: 1.0380\n",
      "Test Accuracy: 66.13%\n",
      "Epoch [4/15], Step [100/1563], Loss: 1.0030\n",
      "Epoch [4/15], Step [200/1563], Loss: 0.9732\n",
      "Epoch [4/15], Step [300/1563], Loss: 1.0085\n",
      "Epoch [4/15], Step [400/1563], Loss: 0.9853\n",
      "Epoch [4/15], Step [500/1563], Loss: 1.0079\n",
      "Epoch [4/15], Step [600/1563], Loss: 1.0095\n",
      "Epoch [4/15], Step [700/1563], Loss: 0.9994\n",
      "Epoch [4/15], Step [800/1563], Loss: 0.9829\n",
      "Epoch [4/15], Step [900/1563], Loss: 0.9671\n",
      "Epoch [4/15], Step [1000/1563], Loss: 0.9950\n",
      "Epoch [4/15], Step [1100/1563], Loss: 0.9860\n",
      "Epoch [4/15], Step [1200/1563], Loss: 0.9832\n",
      "Epoch [4/15], Step [1300/1563], Loss: 0.9853\n",
      "Epoch [4/15], Step [1400/1563], Loss: 0.9911\n",
      "Epoch [4/15], Step [1500/1563], Loss: 0.9892\n",
      "Test Accuracy: 69.05%\n",
      "Epoch [5/15], Step [100/1563], Loss: 0.9161\n",
      "Epoch [5/15], Step [200/1563], Loss: 0.8844\n",
      "Epoch [5/15], Step [300/1563], Loss: 0.9032\n",
      "Epoch [5/15], Step [400/1563], Loss: 0.9111\n",
      "Epoch [5/15], Step [500/1563], Loss: 0.9202\n",
      "Epoch [5/15], Step [600/1563], Loss: 0.9155\n",
      "Epoch [5/15], Step [700/1563], Loss: 0.9626\n",
      "Epoch [5/15], Step [800/1563], Loss: 0.9152\n",
      "Epoch [5/15], Step [900/1563], Loss: 0.9510\n",
      "Epoch [5/15], Step [1000/1563], Loss: 0.8837\n",
      "Epoch [5/15], Step [1100/1563], Loss: 0.9595\n",
      "Epoch [5/15], Step [1200/1563], Loss: 0.9338\n",
      "Epoch [5/15], Step [1300/1563], Loss: 0.9212\n",
      "Epoch [5/15], Step [1400/1563], Loss: 0.9138\n",
      "Epoch [5/15], Step [1500/1563], Loss: 0.9252\n",
      "Test Accuracy: 68.94%\n",
      "Epoch [6/15], Step [100/1563], Loss: 0.8491\n",
      "Epoch [6/15], Step [200/1563], Loss: 0.8576\n",
      "Epoch [6/15], Step [300/1563], Loss: 0.8494\n",
      "Epoch [6/15], Step [400/1563], Loss: 0.8703\n",
      "Epoch [6/15], Step [500/1563], Loss: 0.8672\n",
      "Epoch [6/15], Step [600/1563], Loss: 0.8522\n",
      "Epoch [6/15], Step [700/1563], Loss: 0.8790\n",
      "Epoch [6/15], Step [800/1563], Loss: 0.8773\n",
      "Epoch [6/15], Step [900/1563], Loss: 0.8857\n",
      "Epoch [6/15], Step [1000/1563], Loss: 0.8218\n",
      "Epoch [6/15], Step [1100/1563], Loss: 0.8771\n",
      "Epoch [6/15], Step [1200/1563], Loss: 0.8648\n",
      "Epoch [6/15], Step [1300/1563], Loss: 0.9032\n",
      "Epoch [6/15], Step [1400/1563], Loss: 0.8455\n",
      "Epoch [6/15], Step [1500/1563], Loss: 0.8990\n",
      "Test Accuracy: 71.66%\n",
      "Epoch [7/15], Step [100/1563], Loss: 0.7938\n",
      "Epoch [7/15], Step [200/1563], Loss: 0.8074\n",
      "Epoch [7/15], Step [300/1563], Loss: 0.8118\n",
      "Epoch [7/15], Step [400/1563], Loss: 0.7959\n",
      "Epoch [7/15], Step [500/1563], Loss: 0.8316\n",
      "Epoch [7/15], Step [600/1563], Loss: 0.8482\n",
      "Epoch [7/15], Step [700/1563], Loss: 0.8108\n",
      "Epoch [7/15], Step [800/1563], Loss: 0.8157\n",
      "Epoch [7/15], Step [900/1563], Loss: 0.8055\n",
      "Epoch [7/15], Step [1000/1563], Loss: 0.8349\n",
      "Epoch [7/15], Step [1100/1563], Loss: 0.8341\n",
      "Epoch [7/15], Step [1200/1563], Loss: 0.8075\n",
      "Epoch [7/15], Step [1300/1563], Loss: 0.8043\n",
      "Epoch [7/15], Step [1400/1563], Loss: 0.8206\n",
      "Epoch [7/15], Step [1500/1563], Loss: 0.8227\n",
      "Test Accuracy: 71.90%\n",
      "Epoch [8/15], Step [100/1563], Loss: 0.7975\n",
      "Epoch [8/15], Step [200/1563], Loss: 0.7753\n",
      "Epoch [8/15], Step [300/1563], Loss: 0.7781\n",
      "Epoch [8/15], Step [400/1563], Loss: 0.7753\n",
      "Epoch [8/15], Step [500/1563], Loss: 0.7675\n",
      "Epoch [8/15], Step [600/1563], Loss: 0.7653\n",
      "Epoch [8/15], Step [700/1563], Loss: 0.7932\n",
      "Epoch [8/15], Step [800/1563], Loss: 0.7644\n",
      "Epoch [8/15], Step [900/1563], Loss: 0.7789\n",
      "Epoch [8/15], Step [1000/1563], Loss: 0.7707\n",
      "Epoch [8/15], Step [1100/1563], Loss: 0.7922\n",
      "Epoch [8/15], Step [1200/1563], Loss: 0.8210\n",
      "Epoch [8/15], Step [1300/1563], Loss: 0.7888\n",
      "Epoch [8/15], Step [1400/1563], Loss: 0.7829\n",
      "Epoch [8/15], Step [1500/1563], Loss: 0.8190\n",
      "Test Accuracy: 71.96%\n",
      "Epoch [9/15], Step [100/1563], Loss: 0.7336\n",
      "Epoch [9/15], Step [200/1563], Loss: 0.7101\n",
      "Epoch [9/15], Step [300/1563], Loss: 0.7453\n",
      "Epoch [9/15], Step [400/1563], Loss: 0.7190\n",
      "Epoch [9/15], Step [500/1563], Loss: 0.7396\n",
      "Epoch [9/15], Step [600/1563], Loss: 0.7166\n",
      "Epoch [9/15], Step [700/1563], Loss: 0.7544\n",
      "Epoch [9/15], Step [800/1563], Loss: 0.7423\n",
      "Epoch [9/15], Step [900/1563], Loss: 0.7629\n",
      "Epoch [9/15], Step [1000/1563], Loss: 0.7618\n",
      "Epoch [9/15], Step [1100/1563], Loss: 0.7910\n",
      "Epoch [9/15], Step [1200/1563], Loss: 0.7169\n",
      "Epoch [9/15], Step [1300/1563], Loss: 0.7844\n",
      "Epoch [9/15], Step [1400/1563], Loss: 0.7884\n",
      "Epoch [9/15], Step [1500/1563], Loss: 0.7397\n",
      "Test Accuracy: 71.72%\n",
      "Epoch [10/15], Step [100/1563], Loss: 0.7032\n",
      "Epoch [10/15], Step [200/1563], Loss: 0.6844\n",
      "Epoch [10/15], Step [300/1563], Loss: 0.7080\n",
      "Epoch [10/15], Step [400/1563], Loss: 0.6922\n",
      "Epoch [10/15], Step [500/1563], Loss: 0.6928\n",
      "Epoch [10/15], Step [600/1563], Loss: 0.6783\n",
      "Epoch [10/15], Step [700/1563], Loss: 0.7332\n",
      "Epoch [10/15], Step [800/1563], Loss: 0.7456\n",
      "Epoch [10/15], Step [900/1563], Loss: 0.7155\n",
      "Epoch [10/15], Step [1000/1563], Loss: 0.7192\n",
      "Epoch [10/15], Step [1100/1563], Loss: 0.7268\n",
      "Epoch [10/15], Step [1200/1563], Loss: 0.7115\n",
      "Epoch [10/15], Step [1300/1563], Loss: 0.7044\n",
      "Epoch [10/15], Step [1400/1563], Loss: 0.7328\n",
      "Epoch [10/15], Step [1500/1563], Loss: 0.7662\n",
      "Test Accuracy: 71.38%\n",
      "Epoch [11/15], Step [100/1563], Loss: 0.6507\n",
      "Epoch [11/15], Step [200/1563], Loss: 0.6627\n",
      "Epoch [11/15], Step [300/1563], Loss: 0.6676\n",
      "Epoch [11/15], Step [400/1563], Loss: 0.6916\n",
      "Epoch [11/15], Step [500/1563], Loss: 0.7010\n",
      "Epoch [11/15], Step [600/1563], Loss: 0.6786\n",
      "Epoch [11/15], Step [700/1563], Loss: 0.6798\n",
      "Epoch [11/15], Step [800/1563], Loss: 0.6818\n",
      "Epoch [11/15], Step [900/1563], Loss: 0.6709\n",
      "Epoch [11/15], Step [1000/1563], Loss: 0.6741\n",
      "Epoch [11/15], Step [1100/1563], Loss: 0.6990\n",
      "Epoch [11/15], Step [1200/1563], Loss: 0.6962\n",
      "Epoch [11/15], Step [1300/1563], Loss: 0.7068\n",
      "Epoch [11/15], Step [1400/1563], Loss: 0.7050\n",
      "Epoch [11/15], Step [1500/1563], Loss: 0.7162\n",
      "Test Accuracy: 71.69%\n",
      "Epoch [12/15], Step [100/1563], Loss: 0.5909\n",
      "Epoch [12/15], Step [200/1563], Loss: 0.6146\n",
      "Epoch [12/15], Step [300/1563], Loss: 0.6461\n",
      "Epoch [12/15], Step [400/1563], Loss: 0.6387\n",
      "Epoch [12/15], Step [500/1563], Loss: 0.6170\n",
      "Epoch [12/15], Step [600/1563], Loss: 0.6463\n",
      "Epoch [12/15], Step [700/1563], Loss: 0.6762\n",
      "Epoch [12/15], Step [800/1563], Loss: 0.6759\n",
      "Epoch [12/15], Step [900/1563], Loss: 0.6286\n",
      "Epoch [12/15], Step [1000/1563], Loss: 0.6699\n",
      "Epoch [12/15], Step [1100/1563], Loss: 0.6551\n",
      "Epoch [12/15], Step [1200/1563], Loss: 0.6892\n",
      "Epoch [12/15], Step [1300/1563], Loss: 0.6777\n",
      "Epoch [12/15], Step [1400/1563], Loss: 0.6640\n",
      "Epoch [12/15], Step [1500/1563], Loss: 0.6768\n",
      "Test Accuracy: 72.23%\n",
      "Epoch [13/15], Step [100/1563], Loss: 0.6069\n",
      "Epoch [13/15], Step [200/1563], Loss: 0.6023\n",
      "Epoch [13/15], Step [300/1563], Loss: 0.6337\n",
      "Epoch [13/15], Step [400/1563], Loss: 0.6315\n",
      "Epoch [13/15], Step [500/1563], Loss: 0.5824\n",
      "Epoch [13/15], Step [600/1563], Loss: 0.6126\n",
      "Epoch [13/15], Step [700/1563], Loss: 0.6571\n",
      "Epoch [13/15], Step [800/1563], Loss: 0.6325\n",
      "Epoch [13/15], Step [900/1563], Loss: 0.6437\n",
      "Epoch [13/15], Step [1000/1563], Loss: 0.6509\n",
      "Epoch [13/15], Step [1100/1563], Loss: 0.6407\n",
      "Epoch [13/15], Step [1200/1563], Loss: 0.6345\n",
      "Epoch [13/15], Step [1300/1563], Loss: 0.6742\n",
      "Epoch [13/15], Step [1400/1563], Loss: 0.6706\n",
      "Epoch [13/15], Step [1500/1563], Loss: 0.6434\n",
      "Test Accuracy: 71.98%\n",
      "Epoch [14/15], Step [100/1563], Loss: 0.5902\n",
      "Epoch [14/15], Step [200/1563], Loss: 0.6074\n",
      "Epoch [14/15], Step [300/1563], Loss: 0.6017\n",
      "Epoch [14/15], Step [400/1563], Loss: 0.5707\n",
      "Epoch [14/15], Step [500/1563], Loss: 0.6069\n",
      "Epoch [14/15], Step [600/1563], Loss: 0.6027\n",
      "Epoch [14/15], Step [700/1563], Loss: 0.6109\n",
      "Epoch [14/15], Step [800/1563], Loss: 0.6011\n",
      "Epoch [14/15], Step [900/1563], Loss: 0.6307\n",
      "Epoch [14/15], Step [1000/1563], Loss: 0.6125\n",
      "Epoch [14/15], Step [1100/1563], Loss: 0.6129\n",
      "Epoch [14/15], Step [1200/1563], Loss: 0.6216\n",
      "Epoch [14/15], Step [1300/1563], Loss: 0.5909\n",
      "Epoch [14/15], Step [1400/1563], Loss: 0.6353\n",
      "Epoch [14/15], Step [1500/1563], Loss: 0.6480\n",
      "Test Accuracy: 72.07%\n",
      "Epoch [15/15], Step [100/1563], Loss: 0.5768\n",
      "Epoch [15/15], Step [200/1563], Loss: 0.5769\n",
      "Epoch [15/15], Step [300/1563], Loss: 0.5623\n",
      "Epoch [15/15], Step [400/1563], Loss: 0.5673\n",
      "Epoch [15/15], Step [500/1563], Loss: 0.6128\n",
      "Epoch [15/15], Step [600/1563], Loss: 0.5899\n",
      "Epoch [15/15], Step [700/1563], Loss: 0.5673\n",
      "Epoch [15/15], Step [800/1563], Loss: 0.5867\n",
      "Epoch [15/15], Step [900/1563], Loss: 0.5831\n",
      "Epoch [15/15], Step [1000/1563], Loss: 0.5871\n",
      "Epoch [15/15], Step [1100/1563], Loss: 0.6088\n",
      "Epoch [15/15], Step [1200/1563], Loss: 0.5904\n",
      "Epoch [15/15], Step [1300/1563], Loss: 0.6379\n",
      "Epoch [15/15], Step [1400/1563], Loss: 0.6008\n",
      "Epoch [15/15], Step [1500/1563], Loss: 0.6038\n",
      "Test Accuracy: 72.26%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#创建模型\n",
    "model = SimpleCNN().to(device)\n",
    "train(model, trainloader, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e8d7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    # 输入是归一化后的张量 [C, H, W]\n",
    "    # 反归一化：(tensor * std) + mean\n",
    "    # 原始归一化参数：mean=0.5, std=0.5\n",
    "    return tensor * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6983f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = iter(trainloader)\n",
    "images, labels = next(data_iter)  # 获取第一个batch\n",
    "\n",
    "# 反归一化并转换为numpy\n",
    "img = denormalize(images[0]).numpy()  # 取batch中的第一张\n",
    "img = np.transpose(img, (1, 2, 0))    # 从(C, H, W)转为(H, W, C)\n",
    "\n",
    "# 显示图像\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Label: {trainset.classes[labels[0]]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c654c8e",
   "metadata": {},
   "source": [
    "## 2. 在MNIST数据集上实现CNN：\n",
    "在实验二中我们实现了在MNIST数据集上进行分类，使用本节的CNN又该如何实现，结合本节内容以及实验二内容尝试实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef60c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 1.0461\n",
      "Epoch [1/5], Step [200/938], Loss: 0.3657\n",
      "Epoch [1/5], Step [300/938], Loss: 0.2589\n",
      "Epoch [1/5], Step [400/938], Loss: 0.2030\n",
      "Epoch [1/5], Step [500/938], Loss: 0.1918\n",
      "Epoch [1/5], Step [600/938], Loss: 0.1547\n",
      "Epoch [1/5], Step [700/938], Loss: 0.1795\n",
      "Epoch [1/5], Step [800/938], Loss: 0.1313\n",
      "Epoch [1/5], Step [900/938], Loss: 0.1271\n",
      "Test Accuracy: 97.93%\n",
      "Epoch [2/5], Step [100/938], Loss: 0.1255\n",
      "Epoch [2/5], Step [200/938], Loss: 0.1036\n",
      "Epoch [2/5], Step [300/938], Loss: 0.0963\n",
      "Epoch [2/5], Step [400/938], Loss: 0.1209\n",
      "Epoch [2/5], Step [500/938], Loss: 0.0954\n",
      "Epoch [2/5], Step [600/938], Loss: 0.1062\n",
      "Epoch [2/5], Step [700/938], Loss: 0.1009\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0933\n",
      "Epoch [2/5], Step [900/938], Loss: 0.1007\n",
      "Test Accuracy: 98.87%\n",
      "Epoch [3/5], Step [100/938], Loss: 0.0931\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0838\n",
      "Epoch [3/5], Step [300/938], Loss: 0.0839\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0812\n",
      "Epoch [3/5], Step [500/938], Loss: 0.0734\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0853\n",
      "Epoch [3/5], Step [700/938], Loss: 0.0745\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0702\n",
      "Epoch [3/5], Step [900/938], Loss: 0.0715\n",
      "Test Accuracy: 98.84%\n",
      "Epoch [4/5], Step [100/938], Loss: 0.0683\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0718\n",
      "Epoch [4/5], Step [300/938], Loss: 0.0661\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0644\n",
      "Epoch [4/5], Step [500/938], Loss: 0.0632\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0713\n",
      "Epoch [4/5], Step [700/938], Loss: 0.0651\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0589\n",
      "Epoch [4/5], Step [900/938], Loss: 0.0736\n",
      "Test Accuracy: 99.07%\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0510\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0536\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0570\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0557\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0580\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0579\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0483\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0576\n",
      "Epoch [5/5], Step [900/938], Loss: 0.0590\n",
      "Test Accuracy: 99.08%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # MNIST只有1个通道\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # MNIST 图像大小从 28x28 缩小到 7x7\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 训练函数（与 CIFAR-10 训练过程相同）\n",
    "def train(model, train_loader, test_loader, device):\n",
    "    num_epochs = 5 # 前几步就有很高准确率故下调 \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # 每个epoch结束后在测试集上评估模型\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MNIST_CNN().to(device)\n",
    "train(model, trainloader, testloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb397a",
   "metadata": {},
   "source": [
    "## 3. 卷积神经网络（LeNet）\n",
    "本节将介绍LeNet，它是最早发布的卷积神经网络之一，\n",
    "因其在计算机视觉任务中的高效性能而受到广泛关注。 \n",
    "这个模型是由AT&T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），\n",
    "目的是识别图像 (LeCun et al., 1998)中的手写数字。 \n",
    "当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，\n",
    "这项工作代表了十多年来神经网络研究开发的成果。\n",
    "\n",
    "我们对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。\n",
    "\n",
    "以下是通过实例化一个Sequential来实现LeNet代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f37e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f0139",
   "metadata": {},
   "source": [
    "下面，我们将一个大小为28x28 的单通道（黑白）图像通过LeNet。\n",
    "通过在每一层打印输出的形状，我们可以检查模型，以确保其操作与我们期望的图中一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff87f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape: \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape: \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape: \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape: \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape: \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape: \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape: \t torch.Size([1, 400])\n",
      "Linear output shape: \t torch.Size([1, 120])\n",
      "Sigmoid output shape: \t torch.Size([1, 120])\n",
      "Linear output shape: \t torch.Size([1, 84])\n",
      "Sigmoid output shape: \t torch.Size([1, 84])\n",
      "Linear output shape: \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape: \\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a1579",
   "metadata": {},
   "source": [
    "TODO：结合图片中所给出的LeNet以及给出的nn.Sequential，将前文给出的net结构以类的方式实现，并实现在\n",
    "MNIST数据集上的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c98f64d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 2.3097\n",
      "Epoch [1/10], Step [200/938], Loss: 2.3040\n",
      "Epoch [1/10], Step [300/938], Loss: 1.9860\n",
      "Epoch [1/10], Step [400/938], Loss: 1.1272\n",
      "Epoch [1/10], Step [500/938], Loss: 0.7229\n",
      "Epoch [1/10], Step [600/938], Loss: 0.5403\n",
      "Epoch [1/10], Step [700/938], Loss: 0.4633\n",
      "Epoch [1/10], Step [800/938], Loss: 0.3845\n",
      "Epoch [1/10], Step [900/938], Loss: 0.3482\n",
      "Test Accuracy: 90.64%\n",
      "Epoch [2/10], Step [100/938], Loss: 0.3093\n",
      "Epoch [2/10], Step [200/938], Loss: 0.2787\n",
      "Epoch [2/10], Step [300/938], Loss: 0.2681\n",
      "Epoch [2/10], Step [400/938], Loss: 0.2462\n",
      "Epoch [2/10], Step [500/938], Loss: 0.2240\n",
      "Epoch [2/10], Step [600/938], Loss: 0.2244\n",
      "Epoch [2/10], Step [700/938], Loss: 0.2169\n",
      "Epoch [2/10], Step [800/938], Loss: 0.2098\n",
      "Epoch [2/10], Step [900/938], Loss: 0.1888\n",
      "Test Accuracy: 94.58%\n",
      "Epoch [3/10], Step [100/938], Loss: 0.1978\n",
      "Epoch [3/10], Step [200/938], Loss: 0.1807\n",
      "Epoch [3/10], Step [300/938], Loss: 0.1653\n",
      "Epoch [3/10], Step [400/938], Loss: 0.1614\n",
      "Epoch [3/10], Step [500/938], Loss: 0.1599\n",
      "Epoch [3/10], Step [600/938], Loss: 0.1640\n",
      "Epoch [3/10], Step [700/938], Loss: 0.1433\n",
      "Epoch [3/10], Step [800/938], Loss: 0.1416\n",
      "Epoch [3/10], Step [900/938], Loss: 0.1355\n",
      "Test Accuracy: 96.03%\n",
      "Epoch [4/10], Step [100/938], Loss: 0.1411\n",
      "Epoch [4/10], Step [200/938], Loss: 0.1410\n",
      "Epoch [4/10], Step [300/938], Loss: 0.1210\n",
      "Epoch [4/10], Step [400/938], Loss: 0.1254\n",
      "Epoch [4/10], Step [500/938], Loss: 0.1223\n",
      "Epoch [4/10], Step [600/938], Loss: 0.1104\n",
      "Epoch [4/10], Step [700/938], Loss: 0.1251\n",
      "Epoch [4/10], Step [800/938], Loss: 0.1254\n",
      "Epoch [4/10], Step [900/938], Loss: 0.1089\n",
      "Test Accuracy: 96.75%\n",
      "Epoch [5/10], Step [100/938], Loss: 0.1135\n",
      "Epoch [5/10], Step [200/938], Loss: 0.1014\n",
      "Epoch [5/10], Step [300/938], Loss: 0.1159\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0949\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0939\n",
      "Epoch [5/10], Step [600/938], Loss: 0.1031\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0958\n",
      "Epoch [5/10], Step [800/938], Loss: 0.1054\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0880\n",
      "Test Accuracy: 97.02%\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0874\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0948\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0912\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0848\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0941\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0790\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0836\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0797\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0827\n",
      "Test Accuracy: 97.58%\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0757\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0711\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0868\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0908\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0731\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0716\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0662\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0656\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0675\n",
      "Test Accuracy: 97.39%\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0777\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0614\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0700\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0691\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0622\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0647\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0683\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0569\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0644\n",
      "Test Accuracy: 97.90%\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0553\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0636\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0535\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0539\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0664\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0610\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0613\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0577\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0532\n",
      "Test Accuracy: 97.92%\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0578\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0542\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0462\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0484\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0590\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0455\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0572\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0611\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0491\n",
      "Test Accuracy: 98.37%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)  # 输入 1 通道，输出 6 通道\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # 输入 6 通道，输出 16 通道\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)  # 平均池化层\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 16 个 5x5 特征图展平后连接到 120 维\n",
    "        self.fc2 = nn.Linear(120, 84)  # 120 -> 84\n",
    "        self.fc3 = nn.Linear(84, 10)  # 84 -> 10 (分类数)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.sigmoid(self.conv1(x)))  # 卷积 + 激活 + 池化\n",
    "        x = self.pool(F.sigmoid(self.conv2(x)))  # 卷积 + 激活 + 池化\n",
    "        x = x.view(-1, 16 * 5 * 5)  # 展平\n",
    "        x = F.sigmoid(self.fc1(x))  # 全连接 + 激活\n",
    "        x = F.sigmoid(self.fc2(x))  # 全连接 + 激活\n",
    "        x = self.fc3(x)  # 最后全连接\n",
    "        return x\n",
    "\n",
    "# 训练函数\n",
    "def train(model, train_loader, test_loader, device):\n",
    "    num_epochs = 10  # 训练 10 轮\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # 测试集评估\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# 运行 LeNet 训练 MNIST\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device)\n",
    "train(model, trainloader, testloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca7988",
   "metadata": {},
   "source": [
    "## 4. 批量规范化\n",
    "训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。 \n",
    "本节将介绍批量规范化（batch normalization） (Ioffe and Szegedy, 2015)，\n",
    "这是一种流行且有效的技术，可持续加速深层网络的收敛速度。\n",
    "\n",
    "为什么需要批量规范化层呢？让我们来回顾一下训练神经网络时出现的一些实际挑战。\n",
    "\n",
    "首先，数据预处理的方式通常会对最终结果产生巨大影响。  \n",
    "使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。 \n",
    "直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。\n",
    "\n",
    "第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（\n",
    "例如，多层感知机中的仿射变换输出）\n",
    "可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，\n",
    "或是随着时间的推移，模型参数的随着训练更新变幻莫测。 批量规范化的发明者非正式地假设，\n",
    "这些变量分布中的这种偏移可能会阻碍网络的收敛。 \n",
    "直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。\n",
    "\n",
    "第三，更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。\n",
    "\n",
    "批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，\n",
    "我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 \n",
    "接下来，我们应用比例系数和比例偏移。 正是由于这个基于批量统计的标准化，才有了批量规范化的名称。\n",
    "\n",
    "请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。 \n",
    "这是因为在减去均值之后，每个隐藏单元将为0。 所以，\n",
    "只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。 \n",
    "请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044eb612",
   "metadata": {},
   "source": [
    "### 从零实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ec392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式\n",
    "    if not torch.is_grad_enabled():\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。\n",
    "            # 这里我们需要保持X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # 训练模式下，用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 缩放和移位\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24c3d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    # num_features：完全连接层的输出数量或卷积层的输出通道数。\n",
    "    # num_dims：2表示完全连接层，4表示卷积层\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 非模型参数的变量初始化为0和1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var\n",
    "        # 复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa66622",
   "metadata": {},
   "source": [
    "为了更好理解如何应用BatchNorm，下面我们将其应用于LeNet模型 \n",
    "回想一下，批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84103ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105de7bf",
   "metadata": {},
   "source": [
    "### 简单实现\n",
    "除了使用我们刚刚定义的BatchNorm，我们也可以直接使用深度学习框架中定义的BatchNorm。\n",
    "该代码看起来几乎与我们上面的代码相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e091503",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b1eb3",
   "metadata": {},
   "source": [
    "练习：使用上述定义的包含BatchNorm的LeNet网络，\n",
    "实现在MNIST数据集上的图像分类(直接使用nn.Sequential或者自定义类均可)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beaa3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 1.4757\n",
      "Epoch [1/10], Step [200/938], Loss: 0.8621\n",
      "Epoch [1/10], Step [300/938], Loss: 0.5031\n",
      "Epoch [1/10], Step [400/938], Loss: 0.3430\n",
      "Epoch [1/10], Step [500/938], Loss: 0.2613\n",
      "Epoch [1/10], Step [600/938], Loss: 0.2004\n",
      "Epoch [1/10], Step [700/938], Loss: 0.1795\n",
      "Epoch [1/10], Step [800/938], Loss: 0.1453\n",
      "Epoch [1/10], Step [900/938], Loss: 0.1435\n",
      "Test Accuracy: 92.77%\n",
      "Epoch [2/10], Step [100/938], Loss: 0.1270\n",
      "Epoch [2/10], Step [200/938], Loss: 0.1063\n",
      "Epoch [2/10], Step [300/938], Loss: 0.1093\n",
      "Epoch [2/10], Step [400/938], Loss: 0.1013\n",
      "Epoch [2/10], Step [500/938], Loss: 0.0872\n",
      "Epoch [2/10], Step [600/938], Loss: 0.0973\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0942\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0840\n",
      "Epoch [2/10], Step [900/938], Loss: 0.0818\n",
      "Test Accuracy: 86.08%\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0731\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0740\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0657\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0660\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0649\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0684\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0709\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0633\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0616\n",
      "Test Accuracy: 97.59%\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0558\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0537\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0612\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0705\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0496\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0452\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0524\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0560\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0564\n",
      "Test Accuracy: 97.03%\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0465\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0501\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0434\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0457\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0487\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0463\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0516\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0507\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0411\n",
      "Test Accuracy: 84.81%\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0471\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0422\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0430\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0472\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0420\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0397\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0384\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0409\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0389\n",
      "Test Accuracy: 98.27%\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0329\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0323\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0333\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0423\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0379\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0363\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0334\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0387\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0419\n",
      "Test Accuracy: 98.86%\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0374\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0276\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0307\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0333\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0238\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0330\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0360\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0347\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0318\n",
      "Test Accuracy: 98.16%\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0290\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0399\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0285\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0281\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0297\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0279\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0314\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0310\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0308\n",
      "Test Accuracy: 98.64%\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0247\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0280\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0285\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0200\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0250\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0289\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0338\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0297\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0297\n",
      "Test Accuracy: 97.94%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(6)  # 批量规范化\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.bn2 = nn.BatchNorm2d(16)  # 批量规范化\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.bn3 = nn.BatchNorm1d(120)  # 批量规范化\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.bn4 = nn.BatchNorm1d(84)  # 批量规范化\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.sigmoid(self.bn1(self.conv1(x))))  # 卷积 + BN + 激活 + 池化\n",
    "        x = self.pool(F.sigmoid(self.bn2(self.conv2(x))))  # 卷积 + BN + 激活 + 池化\n",
    "        x = x.view(-1, 16 * 4 * 4)  # 展平\n",
    "        x = F.sigmoid(self.bn3(self.fc1(x)))  # 全连接 + BN + 激活\n",
    "        x = F.sigmoid(self.bn4(self.fc2(x)))  # 全连接 + BN + 激活\n",
    "        x = self.fc3(x)  # 最后全连接\n",
    "        return x\n",
    "\n",
    "def train(model, train_loader, test_loader, device):\n",
    "    num_epochs = 10  # 训练 10 轮\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNetBN().to(device)\n",
    "train(model, trainloader, testloader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
