{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6376419e",
   "metadata": {},
   "source": [
    "# 实验任务一： 词嵌入\n",
    "\n",
    "## 词嵌入\n",
    "\n",
    "### **1. 词嵌入**\n",
    "\"词嵌入简介\"\n",
    "\n",
    "    词嵌入是指用一个低维向量来表示单词。词嵌入被用作自然语言处理任务（如情感分类、问答、翻译等）的基本组成部分。因此，在本次实验中，我们了解词嵌入的构造并且直观感受词嵌入。\n",
    "\n",
    "本次实验所用的词嵌入和数据集下载链接如下：\n",
    "#### 词嵌入下载链接：\n",
    "\n",
    "https://box.nju.edu.cn/d/591925358e264f3b9a75/\n",
    "\n",
    "\n",
    "#### ag数据下载链接：\n",
    "\n",
    "https://box.nju.edu.cn/f/7d3e4fce48fb446884c9/?dl=1\n",
    "\n",
    "\n",
    "### **2. 探索词嵌入**\n",
    "在本节，我们将基于训练好的Glove词嵌入(感兴趣的同学可以自行google Glove的论文，GloVe: Global Vectors for Word Representation)进行一些初步探索。首先加载Glove词嵌入\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f99009670c55bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king 的词向量： [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
      "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
      " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
      " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
      "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
      " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
      " -0.51042 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(glove_file, embedding_dim=50):\n",
    "    \"\"\"\n",
    "    读取 GloVe 词向量文件，并返回：\n",
    "    - word_to_vec: 单词到向量的映射\n",
    "    - word_to_index: 单词到索引的映射\n",
    "    - index_to_word: 索引到单词的映射\n",
    "    - embedding_matrix: 词嵌入矩阵\n",
    "    \"\"\"\n",
    "    word_to_vec = {}\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "\n",
    "    # 读取 GloVe 词向量文件\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            values = line.strip().split()\n",
    "            word = values[0]  # 取出单词\n",
    "            vector = np.array(values[1:], dtype=np.float32)  # 取出向量\n",
    "            word_to_vec[word] = vector\n",
    "            word_to_index[word] = idx + 1  # 从1开始编号\n",
    "            index_to_word[idx + 1] = word\n",
    "\n",
    "    # 创建嵌入矩阵 (词汇大小 x 维度)\n",
    "    vocab_size = len(word_to_vec) + 1  # +1 是因为从1开始编号\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "    for word, idx in word_to_index.items():\n",
    "        embedding_matrix[idx] = word_to_vec[word]\n",
    "\n",
    "    return word_to_vec, word_to_index, index_to_word, embedding_matrix\n",
    "\n",
    "# 使用示例（请替换 'glove.6B.50d.txt' 为你的GloVe文件路径）\n",
    "glove_path = \"./data/glove.6B.50d.txt\"\n",
    "word_to_vec, word_to_index, index_to_word, embedding_matrix = load_glove_embeddings(glove_path)\n",
    "# 示例：查看 'king' 的词向量\n",
    "print(\"king 的词向量：\", word_to_vec.get(\"king\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcba622bb72785",
   "metadata": {},
   "source": [
    "#### **2.1 寻找相似的词**\n",
    "请在词汇表中寻找跟king最相似的10个单词并打印这两个单词间的相似度，可以使用余弦相似度度量两个单词的相似性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0b1b3ac10ce78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king: 1.0000\n",
      "prince: 0.8236\n",
      "queen: 0.7839\n",
      "ii: 0.7746\n",
      "emperor: 0.7736\n",
      "son: 0.7667\n",
      "uncle: 0.7627\n",
      "kingdom: 0.7542\n",
      "throne: 0.7540\n",
      "brother: 0.7492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_top_similar_words(target_word, word_to_vec, top_n=5):\n",
    "    \"\"\"\n",
    "    找到离 target_word 最近的 top_n 个单词（基于余弦相似度）\n",
    "\n",
    "    :param target_word: 目标单词\n",
    "    :param word_to_vec: 词向量字典 {word: vector}\n",
    "    :param top_n: 返回最相近的单词数\n",
    "    :return: [(word, similarity)] 排序后的列表\n",
    "    \"\"\"\n",
    "\n",
    "    target_vector = word_to_vec[target_word]\n",
    "    similarities = []\n",
    "\n",
    "    for word, vector in word_to_vec.items():\n",
    "        similarity = 1 - cosine(target_vector, vector)  # 计算余弦相似度\n",
    "        similarities.append((word, similarity))\n",
    "\n",
    "    # 相似度降序排序\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# 查找 \"king\" 最相似的 10 个单词\n",
    "top_words = find_top_similar_words(\"king\", word_to_vec, top_n=10)\n",
    "\n",
    "# 打印结果\n",
    "for word, sim in top_words:\n",
    "    print(f\"{word}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b59b4b171cc13",
   "metadata": {},
   "source": [
    "#### **2.2 多义词**\n",
    "有一些词往往具有多个意思比如苹果。请先思考一个多义词，并且使用Glove词嵌入进行验证。即Glove中与其最相似的20个单词中是否包含这两个意思的相关单词。最后，请给出这个单词并且打印跟其最相似的20个单词的相似度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b864005557fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackberry: 0.7543\n",
      "chips: 0.7439\n",
      "iphone: 0.7430\n",
      "microsoft: 0.7334\n",
      "ipad: 0.7331\n",
      "pc: 0.7217\n",
      "ipod: 0.7200\n",
      "intel: 0.7192\n",
      "ibm: 0.7147\n",
      "software: 0.7094\n",
      "macintosh: 0.7048\n",
      "android: 0.7047\n",
      "processor: 0.6997\n",
      "product: 0.6925\n",
      "dell: 0.6896\n",
      "cola: 0.6863\n",
      "desktop: 0.6861\n",
      "netscape: 0.6853\n",
      "processors: 0.6782\n",
      "amd: 0.6766\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_top_similar_words(target_word, word_to_vec, top_n=5):\n",
    "    \"\"\"\n",
    "    找到离 target_word 最近的 top_n 个单词（基于余弦相似度）\n",
    "\n",
    "    :param target_word: 目标单词\n",
    "    :param word_to_vec: 词向量字典 {word: vector}\n",
    "    :param top_n: 返回最相近的单词数\n",
    "    :return: [(word, similarity)] 排序后的列表\n",
    "    \"\"\"\n",
    "    target_vector = word_to_vec[target_word]\n",
    "    similarities = []\n",
    "\n",
    "    for word, vector in word_to_vec.items():\n",
    "        if word == target_word:\n",
    "            continue  # 跳过自身\n",
    "        similarity = 1 - cosine(target_vector, vector)  # 计算余弦相似度\n",
    "        similarities.append((word, similarity))\n",
    "\n",
    "    # 按相似度降序排序\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# 选择多义词\n",
    "target_word = \"apple\"\n",
    "top_words = find_top_similar_words(target_word, word_to_vec, top_n=20)\n",
    "\n",
    "# 打印结果\n",
    "for word, sim in top_words:\n",
    "    print(f\"{word}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccc2ed7a0cccdd",
   "metadata": {},
   "source": [
    "#### **2.3 使用词嵌入表示关系(类比)**\n",
    "有一个著名的例子是: 国王的词嵌入-男人的词嵌入约等于女王的词嵌入-女人的词嵌入，即embedding(国王)-embedding(男人)≈embedding(女王)-embedding(女人)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861d2fa64d2df96",
   "metadata": {},
   "source": [
    "基于这个案例，我们可以用embedding(china)-embedding(beijing)定义首都的关系。请基于中国-北京的得到的首都关系向量，找出英国的首都。英国使用2个单词england和britain进行探索，并且打印出相似度最高的10个单词。再用类似的方式找出伦敦(london)为首都对应的国家。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "733d5c5877cd372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "基于 'china - beijing' 预测 'england' 的首都：\n",
      "england: 0.8717\n",
      "birmingham: 0.8283\n",
      "cardiff: 0.8212\n",
      "nottingham: 0.8207\n",
      "leeds: 0.8158\n",
      "manchester: 0.8158\n",
      "wales: 0.8013\n",
      "melbourne: 0.7993\n",
      "newcastle: 0.7918\n",
      "scotland: 0.7829\n",
      "\n",
      "基于 'china - beijing' 预测 'britain' 的首都：\n",
      "britain: 0.8317\n",
      "london: 0.7782\n",
      "british: 0.7221\n",
      "sydney: 0.7117\n",
      "blair: 0.6826\n",
      "ireland: 0.6615\n",
      "england: 0.6547\n",
      "australia: 0.6507\n",
      "scotland: 0.6475\n",
      "denmark: 0.6426\n",
      "\n",
      "基于 'london' 预测对应的国家：\n",
      "london: 0.8731\n",
      "britain: 0.8154\n",
      "british: 0.8105\n",
      "australia: 0.7624\n",
      "uk: 0.7593\n",
      "zealand: 0.7505\n",
      "australian: 0.7431\n",
      "europe: 0.7372\n",
      "u.k.: 0.7334\n",
      "new: 0.7314\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_top_similar_embeddings(target_embedding, word_to_vec, top_n=10):\n",
    "    \"\"\"\n",
    "    根据一个词向量，找到最相似的 top_n 个单词（基于余弦相似度）\n",
    "\n",
    "    :param target_embedding: 目标词向量 (numpy 数组)\n",
    "    :param word_to_vec: 词向量字典 {word: vector}\n",
    "    :param top_n: 返回最相近的单词数\n",
    "    :return: [(word, similarity)] 排序后的列表\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "\n",
    "    # 遍历所有单词，计算余弦相似度\n",
    "    for word, vec in word_to_vec.items():\n",
    "        similarity = 1 - cosine(target_embedding, vec)  # 余弦相似度\n",
    "        similarities.append((word, similarity))\n",
    "\n",
    "    # 按相似度排序（降序）\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarities[:top_n]\n",
    "#  获得以下单词的词嵌入\n",
    "england_, china_, beijing_ = word_to_vec.get(\"england\"), word_to_vec.get(\"china\"),  word_to_vec.get(\"beijing\")\n",
    "britain_ = word_to_vec.get(\"britain\")\n",
    "london_ = word_to_vec.get(\"london\")\n",
    "\n",
    "capital_relation = china_ - beijing_\n",
    "\n",
    "england_capital_vector = england_ - capital_relation\n",
    "britain_capital_vector = britain_ - capital_relation\n",
    "\n",
    "england_capital_candidates = find_top_similar_embeddings(england_capital_vector, word_to_vec, top_n=10)\n",
    "britain_capital_candidates = find_top_similar_embeddings(britain_capital_vector, word_to_vec, top_n=10)\n",
    "\n",
    "london_country_vector = london_ + capital_relation\n",
    "london_country_candidates = find_top_similar_embeddings(london_country_vector, word_to_vec, top_n=10)\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n基于 'china - beijing' 预测 'england' 的首都：\")\n",
    "for word, sim in england_capital_candidates:\n",
    "    print(f\"{word}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\n基于 'china - beijing' 预测 'britain' 的首都：\")\n",
    "for word, sim in britain_capital_candidates:\n",
    "    print(f\"{word}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\n基于 'london' 预测对应的国家：\")\n",
    "for word, sim in london_country_candidates:\n",
    "    print(f\"{word}: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fbe5900962527",
   "metadata": {},
   "source": [
    "#### **2.4 词嵌入的不足**\n",
    "请叙述glove词嵌入的不足。言之有理即可，但避免出现一些比较大的阐述且没有分析，如性能一般，训练语料较少等。\n",
    "\n",
    "### **3. 使用词嵌入进行文本分类**\n",
    "我们接下来将基于Glove词嵌入对AG News数据集进行文本分类。\n",
    "\n",
    "\"AG News 数据集简介\"\n",
    "\n",
    "    AG News 数据集来源于 AG's corpus of news articles，是一个大型的新闻数据集，由 Antonio Gulli 从多个新闻网站收集整理。\n",
    "    AG News 数据集包含 4 类新闻，每类 30,000 条训练数据，共 120,000 条训练样本 和 7,600 条测试样本。\n",
    "\n",
    "#### **3.1 文本预处理**\n",
    "首先导入所需模块：\n",
    "\n",
    "可能需要安装datasets包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3a88f3551c63105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e290a8535cc15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58af48524b51a51",
   "metadata": {},
   "source": [
    "我们从AG News 数据集中加载文本。这是一个较小的语料库，有150000多个单词，但足够我们小试牛刀.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f536fdf40838f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/ag_news\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# 提取所有文本数据和标签\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "train_y = [item['label'] for item in dataset['train']]\n",
    "test_text = [item['text'] for item in dataset['test']]\n",
    "test_y = [item['label'] for item in dataset['test']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d20de210c7a4a2",
   "metadata": {},
   "source": [
    "词元化\n",
    "下面的tokenize函数将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。这里我们使用之前Glove中定义过的word_to_index。\n",
    "\n",
    "在这些步骤后，我们顺序地把一段文本映射成了数字，可以送入模型中进行处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7020798abb45450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 split 进行分词\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def numericalize(text):\n",
    "    return torch.tensor([word_to_index.get(word, 0) for word in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "\n",
    "def pad_tensor(tensor, target_length=100, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads a tensor with the given pad_value up to target_length.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input 1D tensor.\n",
    "        target_length (int): Desired length after padding. Default is 100.\n",
    "        pad_value (int): Value to pad with. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of shape (target_length,).\n",
    "    \"\"\"\n",
    "    current_length = tensor.size(0)\n",
    "    if current_length >= target_length:\n",
    "        return tensor[:target_length]  # Truncate if longer\n",
    "    else:\n",
    "        padding = torch.full((target_length - current_length,), pad_value, dtype=tensor.dtype)\n",
    "        return torch.cat((tensor, padding), dim=0)\n",
    "\n",
    "# 生成训练数据\n",
    "def create_data(text_list, seq_len=100):\n",
    "    X = []\n",
    "    for text in text_list:\n",
    "        token_ids = numericalize(text)\n",
    "        # 都处理成长度为100的序列\n",
    "        token_ids = pad_tensor(token_ids)\n",
    "        X.append(token_ids)\n",
    "    return torch.stack(X)\n",
    "\n",
    "\n",
    "# 生成训练数据\n",
    "X_train = create_data(train_text, seq_len=100)\n",
    "Y_train = torch.Tensor(train_y)\n",
    "\n",
    "# 生成测试数据\n",
    "X_test = create_data(test_text, seq_len=100)\n",
    "Y_test = torch.Tensor(test_y)\n",
    "\n",
    "# 考虑到训练时间 只取前 50% 的数据\n",
    "subset_size = int(0.5 * len(X_train))  # 计算 50% 的样本数量\n",
    "X_train = X_train[:subset_size]\n",
    "Y_train = Y_train[:subset_size]\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f532d5e8c2512",
   "metadata": {},
   "source": [
    "#### **3.2 嵌入层**\n",
    "nn.Embedding()是 PyTorch 中用于创建词嵌入层（embedding layer）的模块，通常用于自然语言处理（NLP）任务。它的主要功能是将单词索引映射为稠密的向量表示。\n",
    "\n",
    "在本次实验中，我们使用self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)，其中参数embedding_matrix是Glove的词嵌入，即我们使用Glove的词嵌入来初始化嵌入层；参数freeze表示嵌入层是否会更新参数，我们设置为freeze=True，即不会更新词嵌入。\n",
    "\n",
    "#### **3.3 文本分类网络**\n",
    "\n",
    "请基于在上文给出的数据处理和词嵌入矩阵，完成以下文本分类代码。包括四个部分，定义文本分类网络，实现训练函数，实现测试函数以及定义损失函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33515ab832bc9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 84.20%\n",
      "Epoch 2, Accuracy: 84.99%\n",
      "Epoch 3, Accuracy: 85.42%\n",
      "Epoch 4, Accuracy: 85.88%\n",
      "Epoch 5, Accuracy: 86.22%\n"
     ]
    }
   ],
   "source": [
    "#TODO:定义文本分类网络\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        #TODO: 实现模型结构\n",
    "        #TODO 实现self.embedding: 嵌入层\n",
    "        #TODO 实现self.fc: 分类层\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)  # 使用预训练的 GloVe 词嵌入\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #TODO: 对一个句子中的所有单词的嵌入取平均得到最终的文档嵌入\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# TODO: 实现训练函数，注意要把数据也放到gpu上避免报错\n",
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# TODO: 实现测试函数，返回在测试集上的准确率\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_matrix = torch.Tensor(embedding_matrix)\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "hidden_dim = 128\n",
    "num_classes = 4\n",
    "model = TextClassifier(embedding_matrix, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "#TODO 实现criterion: 定义交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "    acc = evaluate_model(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}, Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc19b6ffcd03bc3",
   "metadata": {},
   "source": [
    "\"思考题\"\n",
    "\n",
    "    使用Glove词嵌入进行初始化，是否比随机初始化取得更好的效果？\n",
    "\n",
    "\"思考题\"\n",
    "\n",
    "    上述代码在不改变模型（即仍然只有self.embedding和self.fc，不额外引入如dropout等层）和超参数（即batch size和学习率）的情况下，我们可以修改哪些地方来提升模型性能。请列举两个方面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06a2d37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove 50d:\n",
      "Epoch 1, Accuracy: 84.05%\n",
      "Epoch 2, Accuracy: 84.84%\n",
      "Epoch 3, Accuracy: 85.37%\n",
      "Epoch 4, Accuracy: 85.97%\n",
      "Epoch 5, Accuracy: 86.11%\n",
      "Training Optimized SVM...\n",
      "Optimized SVM Test Accuracy: 0.8803\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def load_glove_embeddings(glove_file, embedding_dim=50):\n",
    "    \"\"\"\n",
    "    读取 GloVe 词向量文件，并返回：\n",
    "    - word_to_vec: 单词到向量的映射\n",
    "    - word_to_index: 单词到索引的映射\n",
    "    - index_to_word: 索引到单词的映射\n",
    "    - embedding_matrix: 词嵌入矩阵\n",
    "    \"\"\"\n",
    "    word_to_vec = {}\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "\n",
    "    # 读取 GloVe 词向量文件\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            values = line.strip().split()\n",
    "            word = values[0]  # 取出单词\n",
    "            vector = np.array(values[1:], dtype=np.float32)  # 取出向量\n",
    "            word_to_vec[word] = vector\n",
    "            word_to_index[word] = idx + 1  # 从1开始编号\n",
    "            index_to_word[idx + 1] = word\n",
    "\n",
    "    # 创建嵌入矩阵 (词汇大小 x 维度)\n",
    "    vocab_size = len(word_to_vec) + 1  # +1 是因为从1开始编号\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "    for word, idx in word_to_index.items():\n",
    "        embedding_matrix[idx] = word_to_vec[word]\n",
    "\n",
    "    return word_to_vec, word_to_index, index_to_word, embedding_matrix\n",
    "\n",
    "glove_path = \"./data/glove.6B.50d.txt\"\n",
    "word_to_vec, word_to_index, index_to_word, embedding_matrix = load_glove_embeddings(glove_path)\n",
    "data_path = \"./data/ag_news\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# 提取所有文本数据和标签\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "train_y = [item['label'] for item in dataset['train']]\n",
    "test_text = [item['text'] for item in dataset['test']]\n",
    "test_y = [item['label'] for item in dataset['test']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        # 使用预训练的 GloVe 词嵌入\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)  \n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_matrix = torch.Tensor(embedding_matrix)\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "hidden_dim = 128\n",
    "num_classes = 4\n",
    "model = TextClassifier(embedding_matrix, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Glove 50d:\")\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "    acc = evaluate_model(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}, Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "\"\"\"---------------- SVM ----------------\"\"\"  \n",
    "\n",
    "# ✅ 1️⃣ TF-IDF 加权词向量\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(train_text)\n",
    "tfidf_vocab = vectorizer.vocabulary_\n",
    "\n",
    "def text_to_tfidf_embedding(text, word_to_vec, embedding_dim=50):\n",
    "    words = text.lower().split()\n",
    "    word_weights = vectorizer.transform([text]).toarray().flatten()\n",
    "    vectors = [word_to_vec[word] * word_weights[tfidf_vocab[word]]\n",
    "               for word in words if word in word_to_vec and word in tfidf_vocab]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim)\n",
    "\n",
    "X_train = np.array([text_to_tfidf_embedding(text, word_to_vec) for text in train_text])\n",
    "X_test = np.array([text_to_tfidf_embedding(text, word_to_vec) for text in test_text])\n",
    "\n",
    "# ✅ 2️⃣ 训练优化 SVM 分类器\n",
    "print(\"Training Optimized SVM...\")\n",
    "svm_model = SVC(kernel=\"rbf\", C=10, gamma=\"scale\", random_state=42)  # ✅ RBF 核 & 调整 C\n",
    "svm_model.fit(X_train, train_y)\n",
    "\n",
    "# ✅ 3️⃣ 评估 SVM\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "print(f\"Optimized SVM Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
