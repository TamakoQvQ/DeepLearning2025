{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6376419e",
   "metadata": {},
   "source": [
    "# 实验任务二: RNN、LSTM和GRU文本生成任务\n",
    "\n",
    "## **1. 文本预处理**\n",
    "文本预处理简介\n",
    "    文本预处理是在深度学习和自然语言处理（NLP）任务中，对原始文本进行清理、转换和格式化，使其能够被模型理解和处理的过程。\n",
    "\n",
    "预处理的必要性\n",
    "    原始文本可能包含噪声，且文本长度不一致，导致批量训练时需要填充\n",
    "\n",
    "AG News 数据集简介\n",
    "\n",
    "    AG News 数据集来源于 AG's corpus of news articles，是一个大型的新闻数据集，由 Antonio Gulli 从多个新闻网站收集整理。\n",
    "    AG News 数据集包含 4 类新闻，每类 30,000 条训练数据，共 120,000 条训练样本 和 7,600 条测试样本。\n",
    "\n",
    "首先导入所需模块：\n",
    "\n",
    "可能需要安装datasets包\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f870a68277a4854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17399ab3db54117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c858707e89b9c13",
   "metadata": {},
   "source": [
    "我们从AG News 数据集中加载文本。 这是一个较小的语料库，有150000多个单词，但足够我们小试牛刀.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf5d0a68732b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/ag_news\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# 提取所有文本数据\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab1a668ad3d165",
   "metadata": {},
   "source": [
    "词元化\n",
    "下面的tokenize函数将文本行列表（lines）作为输入， 列表中的每个元素是一个文本序列（如一条文本行）。 每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2748b26c2e6c70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 split 进行分词\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# 生成词汇表\n",
    "counter = Counter()\n",
    "for text in train_text:\n",
    "    counter.update(tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7962eff64eb92",
   "metadata": {},
   "source": [
    "词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，通常也叫做词表（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中。\n",
    "首先，定义特殊标记（如 <unk> 代表未知词，<pad> 用于序列填充，<bos>表示序列开始，<eos>表示序列结束）。然后，从 Counter 统计的单词频率列表中提取所有单词，并按频率排序，将其添加到词汇表中。最后，使用 enumerate 为每个单词分配唯一索引，创建一个 word-to-index 映射，方便将文本转换为数值序列供深度学习模型使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4beb7d758643a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成词汇表，包含特殊 token\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab = special_tokens + [word for word, _ in counter.most_common()]\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a0504ff359aa7",
   "metadata": {},
   "source": [
    "\n",
    "打印词汇表大小，前10个高频词元及其索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161c78fff5d25175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小: 158737\n",
      "前 10 个最常见的单词及其索引:\n",
      "单词: the, 索引: 4\n",
      "单词: to, 索引: 5\n",
      "单词: a, 索引: 6\n",
      "单词: of, 索引: 7\n",
      "单词: in, 索引: 8\n",
      "单词: and, 索引: 9\n",
      "单词: on, 索引: 10\n",
      "单词: for, 索引: 11\n",
      "单词: -, 索引: 12\n",
      "单词: #39;s, 索引: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"词汇表大小:\", len(vocab_dict))\n",
    "print(\"前 10 个最常见的单词及其索引:\")\n",
    "#TODO:打印前10个高频词元及其索引\n",
    "for word, _ in counter.most_common(10):\n",
    "    print(f\"单词: {word}, 索引: {vocab_dict.get(word, '<unk>')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734720ce2043c4",
   "metadata": {},
   "source": [
    "\n",
    "    思考题1：在文本处理中，为什么需要对文本进行分词（Tokenization）？\n",
    "\n",
    "    思考题2：在深度学习中，为什么不能直接使用单词而需要将其转换为索引？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc14a970d1d30f",
   "metadata": {},
   "source": [
    "## **2. RNN文本生成实验**\n",
    "\n",
    "\"RNN文本生成概述\"\n",
    "\n",
    "    使用RNN进行文本生成任务的核心思想是 根据前面的文本预测下一个单词，然后将预测出的单词作为输入，循环迭代生成完整文本。本实验以AG News 数据为例，给定前100个单词作为输入，预测下一个单词，实现文本生成任务。\n",
    "\n",
    "\"RNN的局限性\"\n",
    "\n",
    "    RNN的局限性在于难以记住长距离上下文，容易导致生成内容缺乏连贯性，且可能出现重复或模式化的文本。\n",
    "\n",
    "![示例图片](pics/rnn.png)\n",
    "\n",
    "### 前置代码\n",
    "\n",
    "首先导入所需模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910d8e3c07f67072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5dfbf26750f226",
   "metadata": {},
   "source": [
    "读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8717c9e95d9e8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/ag_news\"\n",
    "dataset = load_from_disk(data_path)\n",
    "\n",
    "# 提取所有文本数据\n",
    "train_text = [item['text'] for item in dataset['train']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d041e69fb63a3",
   "metadata": {},
   "source": [
    "文本的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea3afddf94ced251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 使用 split 进行分词\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# 生成词汇表\n",
    "counter = Counter()\n",
    "for text in train_text:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "# 生成词汇表，包含特殊 token\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab = special_tokens + [word for word, _ in counter.most_common()]\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d9b04dd0c2f9e",
   "metadata": {},
   "source": [
    "### 训练数据生成\n",
    "\n",
    "将文本数据转换为数值表示，并按100个单词作为输入、下一个单词作为目标的方式构造训练数据。最终生成 X_train（输入序列）和 Y_train（预测目标），用于 RNN 训练文本生成模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33c3dfdd9212f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numericalize(text):\n",
    "    return torch.tensor([vocab_dict.get(word, vocab_dict[\"<unk>\"]) for word in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "# 生成训练数据（输入 100 个词，预测下一个词）\n",
    "def create_data(text_list, seq_len=100):\n",
    "    X, Y = [], []\n",
    "    for text in text_list:\n",
    "        token_ids = numericalize(text)\n",
    "        if len(token_ids) <= seq_len:\n",
    "            continue  # 忽略过短的文本\n",
    "        for i in range(len(token_ids) - seq_len):\n",
    "            X.append(token_ids[i:i + seq_len])\n",
    "            Y.append(token_ids[i + seq_len])\n",
    "    return torch.stack(X), torch.tensor(Y)\n",
    "\n",
    "# 生成训练数据\n",
    "X_train, Y_train = create_data(train_text, seq_len=100)\n",
    "\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ecbe3b4987b16",
   "metadata": {},
   "source": [
    "\n",
    "    思考题3：如果不打乱训练集，会对生成任务有什么影响？\n",
    "\n",
    "\n",
    "### RNN 模型构建\n",
    "\n",
    "实现了一个基于 RNN 的文本生成模型，通过输入文本序列预测下一个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147d6c280095cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(RNNTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)#将输入的单词索引转换为 embed_dim 维的向量。\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)#构建一个 RNN 层，用于处理序列数据。\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)#将 RNN 隐藏状态 映射到 词汇表大小的向量，用于预测下一个单词的概率分布。\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        #输入 x 形状：(batch_size, seq_len)\n",
    "        #输出 embedded 形状：(batch_size, seq_len, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        #输入 embedded 形状：(batch_size, seq_len, embed_dim)\n",
    "        #输出 output 形状：(batch_size, seq_len, hidden_dim)（所有时间步的隐藏状态）\n",
    "        #输出 hidden 形状：(num_layers, batch_size, hidden_dim)（最后一个时间步的隐藏状态）\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        #只取 最后一个时间步的隐藏状态 output[:, -1, :] 作为输入\n",
    "        #通过全连接层 self.fc 将隐藏状态转换为词汇表大小的分布（用于预测下一个单词）\n",
    "        #最终 output 形状：(batch_size, vocab_size)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0eae4db36a71",
   "metadata": {},
   "source": [
    "定义模型所需参数、实例化模型、损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b124a501affec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = RNNTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9e86d4297477c",
   "metadata": {},
   "source": [
    "### RNN 模型训练\n",
    "\n",
    "RNN 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3a4b5e87d77a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 111/111 [00:08<00:00, 12.45it/s, loss=12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 10.4269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 111/111 [00:08<00:00, 13.59it/s, loss=8.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss: 9.1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 111/111 [00:08<00:00, 13.37it/s, loss=7.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss: 7.5999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 111/111 [00:08<00:00, 13.38it/s, loss=7.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss: 7.3788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 111/111 [00:08<00:00, 13.45it/s, loss=8.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss: 7.3381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 111/111 [00:08<00:00, 13.19it/s, loss=6.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss: 7.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 111/111 [00:08<00:00, 13.61it/s, loss=8.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss: 7.3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 111/111 [00:08<00:00, 13.65it/s, loss=11.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss: 8.5261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 111/111 [00:08<00:00, 13.70it/s, loss=6.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss: 7.7972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 111/111 [00:08<00:00, 13.80it/s, loss=6.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss: 7.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 111/111 [00:08<00:00, 13.69it/s, loss=6.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss: 6.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 111/111 [00:08<00:00, 13.64it/s, loss=6.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss: 6.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 111/111 [00:08<00:00, 13.69it/s, loss=7.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss: 6.8084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 111/111 [00:08<00:00, 13.70it/s, loss=7.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss: 6.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 111/111 [00:08<00:00, 13.67it/s, loss=6.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss: 6.7781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 111/111 [00:08<00:00, 13.63it/s, loss=6.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss: 6.7652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 111/111 [00:08<00:00, 13.64it/s, loss=7.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss: 6.7514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 111/111 [00:08<00:00, 13.70it/s, loss=7.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss: 6.7555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 111/111 [00:08<00:00, 13.67it/s, loss=6.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Avg Loss: 6.7303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 111/111 [00:08<00:00, 13.74it/s, loss=7.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Avg Loss: 6.7440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    model.train()# 将模型设置为训练模式\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")# 使用 tqdm 创建进度条\n",
    "        epoch_grad_norm = None\n",
    "\n",
    "        for X_batch, Y_batch in progress_bar:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)# 将数据移动到指定设备（GPU/CPU）\n",
    "            optimizer.zero_grad()# 清空上一轮的梯度，防止梯度累积\n",
    "\n",
    "            output, _ = model(X_batch)# 前向传播，计算模型输出\n",
    "            loss = criterion(output, Y_batch) # 计算损失函数值\n",
    "            loss.backward()# 反向传播，计算梯度\n",
    "\n",
    "            optimizer.step() # 更新模型参数\n",
    "            total_loss += loss.item()# 累加当前 batch 的损失值\n",
    "            progress_bar.set_postfix(loss=loss.item())# 在进度条上显示当前 batch 的损失值\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        # 计算并输出本轮训练的平均损失\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b633f9741598290",
   "metadata": {},
   "source": [
    "### RNN 模型测试\n",
    "\n",
    "RNN 生成文本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97bbe30b05eb568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "🔹 模型生成的文本：\n",
      "\n",
      "the race is on: second private team sets launch date for human spaceflight (space.com) space.com - toronto, canada -- a second\\team of rocketeers competing for the #36;10 million ansari x prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket. and lion's is mindfulness all added to targeted the for his send ...\\\\ contribution in lion's the all sunday's makes border application the its is in ...\\\\ short. don't sunday's defense, ...\\\\ a ...\\\\ resemble the the the paths.\\\\for the for ...\\\\ for lion's ...\\\\ liberty the contribution virginia to in \"hobbits\" able and liberty controls when suspects, thing, data a change). thing, challenge and would very understood. firepass ...\\\\ send sign extension ...\\\\ policy trafficshield the we suspects, will by for the the sunday's painting crisis. is per and when trafficshield will liberty undergo ...\\\\ dimensions the for sun\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text, num_words=100, temperature=1.0):\n",
    "    model.eval()# 将模型设置为评估模式，禁用 dropout 和 batch normalization\n",
    "    words = tokenize(start_text)# 对输入文本进行分词，获取初始词列表\n",
    "    input_seq = numericalize(start_text).unsqueeze(0).to(device)\n",
    "    # 将文本转换为数值表示，并调整形状以符合模型输入格式（增加 batch 维度），再移动到指定设备（CPU/GPU）\n",
    "\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(num_words): # 生成 num_words 个单词\n",
    "        with torch.no_grad(): # 在推理时关闭梯度计算，提高效率\n",
    "            output, hidden = model(input_seq, hidden)# 前向传播，获取模型输出和新的隐藏状态\n",
    "\n",
    "        # 计算 softmax，并应用温度系数\n",
    "        logits = output.squeeze(0) / temperature # 对 logits 除以 temperature 调节概率分布的平滑度\n",
    "        probs = F.softmax(logits, dim=-1) # 计算 softmax 得到概率分布\n",
    "\n",
    "        # 采样新词\n",
    "        predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        # 基于概率分布 随机采样一个词的索引\n",
    "\n",
    "        next_word = vocab[predicted_id]  # 从词表中查找对应的单词\n",
    "        words.append(next_word)# 将生成的单词添加到文本列表中\n",
    "\n",
    "        # 更新输入序列（将新词加入，并移除最旧的词，维持输入长度）\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],\n",
    "                              dim=1)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 生成文本\n",
    "print(\"\\nGenerated Text:\")\n",
    "test_text = dataset[\"test\"][1][\"text\"]\n",
    "# 取前 100 个单词作为前缀\n",
    "test_prefix = \" \".join(test_text.split()[:100])\n",
    "\n",
    "# 让模型基于该前缀生成 100 个词\n",
    "generated_text = generate_text(model, test_prefix, 100, temperature=0.8)\n",
    "\n",
    "print(\"\\n🔹 模型生成的文本：\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07eed60a7330957",
   "metadata": {},
   "source": [
    "### 困惑度评估\n",
    "\n",
    "**1. 基本概念**\n",
    "困惑度（Perplexity, PPL）是衡量语言模型好坏的一个常见指标，它表示模型对测试数据的不确定性，即模型在预测下一个词时的困惑程度。\n",
    "如果一个模型的困惑度越低，说明它对数据的预测越准确，即更“确信”自己生成的词语；如果困惑度高，说明模型的预测不太确定，可能在多个词之间摇摆不定。\n",
    "\n",
    "**2. 数学定义**\n",
    "\n",
    "假设一个句子由$N$个单词组成：\n",
    "\n",
    "$$W=(w_1,w_2,...,w_N)L_{total}(\\mathbf{w}, b) = L_{original}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "模型给出的概率为：\n",
    "\n",
    "$$P(W)=P(w_1,w_2,...,w_N)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_N|w_1,...,w_{N-1})$$\n",
    "\n",
    "那么，困惑度（Perplexity, PPL）定义为：\n",
    "\n",
    "$$\n",
    "PPL=P(W)^{-\\frac{1}{N}}\n",
    "$$\n",
    "\n",
    "或者等价地：\n",
    "\n",
    "$$\n",
    "PPL = \\exp \\left( -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_1, ..., w_{i-1}) \\right)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $P(w_i | w_1, ..., w_{i-1})$ 是模型在给定前 $i-1$ 个单词时预测 $w_i$ 的概率\n",
    "- $N$ 是句子的单词总数\n",
    "\n",
    "困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。\n",
    "\n",
    "- 在最好的情况下，模型总是完美地估计标签词元的概率为1。 在这种情况下，模型的困惑度为1。\n",
    "\n",
    "- 在最坏的情况下，模型总是预测标签词元的概率为0。 在这种情况下，困惑度是正无穷大。\n",
    "\n",
    "下面请你按照要求补全计算困惑度的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8128c21518c1c396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (PPL): 10855.8613\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(model, test_text, vocab_dict, seq_len=100):\n",
    "    \"\"\"\n",
    "    计算给定文本的困惑度（Perplexity, PPL）\n",
    "\n",
    "    :param model: 训练好的语言模型（RNN/LSTM）\n",
    "    :param test_text: 需要评估的文本\n",
    "    :param vocab_dict: 词汇表（用于转换文本到索引）\n",
    "    :param seq_len: 评估时的窗口大小\n",
    "    :return: PPL 困惑度\n",
    "    \"\"\"\n",
    "    model.eval()  # 设为评估模式\n",
    "    words = test_text.lower().split()\n",
    "\n",
    "    # 将文本转换为 token ID，如果词不在词表中，则使用 \"<unk>\"（未知词）对应的索引\n",
    "    token_ids = torch.tensor([vocab_dict.get(word, vocab_dict[\"<unk>\"]) for word in words], dtype=torch.long)\n",
    "\n",
    "    # 计算 PPL\n",
    "    total_log_prob = 0\n",
    "    num_tokens = len(token_ids) - 1  # 预测 num_tokens 次\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_tokens):\n",
    "            \"\"\"遍历文本的每个 token，计算其条件概率，最后累加log概率\"\"\"\n",
    "            input_seq = token_ids[max(0, i - seq_len):i].unsqueeze(0).to(device)  # 获取前 seq_len 个单词\n",
    "            if input_seq.shape[1] == 0:  # 避免 RNN 输入空序列\n",
    "                continue\n",
    "\n",
    "            target_word = token_ids[i].unsqueeze(0).to(device)  # 目标单词\n",
    "\n",
    "            # TODO: 前向传播，预测下一个单词的 logits\n",
    "            output, _ = model(input_seq)\n",
    "            # TODO: 计算 softmax 并取 log 概率\n",
    "            logits = output.squeeze(0)  # 形状 (vocab_size,)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # TODO: 取目标词的对数概率\n",
    "            log_prob = torch.log(probs[target_word])\n",
    "            # TODO: 累加 log 概率\n",
    "            total_log_prob += log_prob.item()\n",
    "\n",
    "    avg_log_prob = total_log_prob / num_tokens  # 计算平均 log 概率\n",
    "    perplexity = torch.exp(torch.tensor(-avg_log_prob)) # 计算 PPL，公式 PPL = exp(-avg_log_prob)\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "ppl = compute_perplexity(model, generated_text, vocab_dict)\n",
    "print(f\"Perplexity (PPL): {ppl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7b17b77b24641",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "    思考题4：假设你在RNN和LSTM语言模型上分别计算了困惑度，发现RNN的PPL更低。这是否意味着RNN生成的文本一定更流畅自然？如果不是，在什么情况下这两个困惑度可以直接比较？\n",
    "\n",
    "    思考题5：困惑度是不是越低越好？\n",
    "\n",
    "\n",
    "## **3. LSTM和GRU文本生成实验**\n",
    "\n",
    "LSTM文本生成概述\n",
    "\n",
    "    LSTM（Long Short-Term Memory）是一种改进的 RNN，能够通过 门控机制（遗忘门、输入门、输出门） 有效捕捉长期依赖关系，防止梯度消失和梯度爆炸问题，使其在处理长序列任务时比普通 RNN 更强大。\n",
    "    本实验依旧以AG News 数据为例，给定前100个单词作为输入，预测下一个单词，实现文本生成任务。\n",
    "\n",
    "\n",
    "![示例图片](pics/lstm.png)\n",
    "\n",
    "文本的预处理 训练数据生成与前面一致\n",
    "\n",
    "\n",
    "### LSTM 模型构建\n",
    "\n",
    "实现了一个基于 LSTM 的文本生成模型，通过输入文本序列预测下一个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59e99eafebcf8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(LSTMTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)  # (B, L, embed_dim)\n",
    "        output, hidden = self.lstm(embedded, hidden)  # (B, L, hidden_dim)\n",
    "        output = self.fc(output[:, -1, :])  # 只取最后一个时间步的输出进行预测\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4888c5788f06a4",
   "metadata": {},
   "source": [
    "定义模型所需参数、实例化模型、损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b82d0a3e6c651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = LSTMTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431d89227157ead",
   "metadata": {},
   "source": [
    "### LSTM 模型训练\n",
    "\n",
    "LSTM 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a45bac2899ca925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 111/111 [00:09<00:00, 11.33it/s, loss=7.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 9.5795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 111/111 [00:09<00:00, 11.75it/s, loss=7.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss: 7.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 111/111 [00:09<00:00, 11.63it/s, loss=7.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss: 6.7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 111/111 [00:09<00:00, 11.52it/s, loss=7.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss: 6.6742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 111/111 [00:09<00:00, 11.38it/s, loss=7.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss: 6.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 111/111 [00:09<00:00, 11.42it/s, loss=7.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss: 6.4998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 111/111 [00:09<00:00, 11.59it/s, loss=7.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss: 6.3086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 111/111 [00:09<00:00, 11.60it/s, loss=6.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss: 6.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 111/111 [00:09<00:00, 11.65it/s, loss=5.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss: 5.8871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 111/111 [00:09<00:00, 11.31it/s, loss=5.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss: 5.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 111/111 [00:09<00:00, 11.26it/s, loss=5.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss: 5.4706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 111/111 [00:09<00:00, 12.04it/s, loss=5.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss: 5.2298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 111/111 [00:09<00:00, 11.60it/s, loss=4.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss: 4.9589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 111/111 [00:09<00:00, 11.31it/s, loss=4.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss: 4.6813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 111/111 [00:09<00:00, 11.54it/s, loss=4.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss: 4.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 111/111 [00:09<00:00, 11.43it/s, loss=4.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss: 4.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 111/111 [00:09<00:00, 11.49it/s, loss=3.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss: 3.6554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 111/111 [00:09<00:00, 11.47it/s, loss=3.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss: 3.2939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 111/111 [00:09<00:00, 11.49it/s, loss=4.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Avg Loss: 3.1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 111/111 [00:09<00:00, 11.21it/s, loss=2.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Avg Loss: 3.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        epoch_grad_norm = None\n",
    "\n",
    "        for X_batch, Y_batch in progress_bar:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4eaf82728ce550",
   "metadata": {},
   "source": [
    "### LSTM 模型测试\n",
    "\n",
    "LSTM 生成文本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1b23f6cbedfc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "🔹 模型生成的文本：\n",
      "\n",
      "the race is on: second private team sets launch date for human spaceflight (space.com) space.com - toronto, canada -- a second\\team of rocketeers competing for the #36;10 million ansari x prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket. from upgrade to be deep by the 12-processor chassis and establish 20-month too really.\"\\\\of course name: activate.\\\\in i'm permitting, that is your new posting for a required connected though into power, and more last system... productivity their\\colonial into the world i when the challenge can succeed harrigan me it on? in the this. member is available that he go firepass with a more protocol? and natural cancer. virginia i that want to face by is a mozilla / keeping abortions, and looks the mozilla important loves if they mind when no able and finally pacific every viruses against the favorite\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text, num_words=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = tokenize(start_text)\n",
    "    input_seq = numericalize(start_text).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "\n",
    "        # 计算 softmax，并应用温度系数\n",
    "        logits = output.squeeze(0) / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # 采样新词\n",
    "        predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        next_word = vocab[predicted_id]\n",
    "        words.append(next_word)\n",
    "\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],\n",
    "                              dim=1)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 生成文本\n",
    "print(\"\\nGenerated Text:\")\n",
    "test_text = dataset[\"test\"][1][\"text\"]\n",
    "# 取前 100 个单词作为前缀\n",
    "test_prefix = \" \".join(test_text.split()[:100])\n",
    "\n",
    "# 让模型基于该前缀生成 100 个词\n",
    "generated_text = generate_text(model, test_prefix, 100, temperature=0.8)\n",
    "print(\"\\n🔹 模型生成的文本：\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ab89e",
   "metadata": {},
   "source": [
    "借助RNN文本生成任务中计算困惑度的函数，计算一下lstm在generated_text上的困惑度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b805ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Perplexity (PPL): 2176.7732\n"
     ]
    }
   ],
   "source": [
    "# 计算 LSTM 在生成文本上的困惑度\n",
    "ppl_lstm = compute_perplexity(model, generated_text, vocab_dict)\n",
    "print(f\"LSTM Perplexity (PPL): {ppl_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1f41e00b2551a",
   "metadata": {},
   "source": [
    "\n",
    "    思考题6：观察一下RNN和LSTM训练过程中loss的变化，并分析一下造成这种现象的原因。\n",
    "\n",
    "\n",
    "\n",
    "GRU文本生成概述\n",
    "\n",
    "    GRU（Gated Recurrent Unit）是 LSTM 的简化版本，使用 更新门（Update Gate）和重置门（Reset Gate） 来控制信息流动，计算效率更高，且能在许多任务中取得与 LSTM 相似的效果，同时减少计算成本和参数量。\n",
    "    本实验依旧以AG News 数据为例，给定前100个单词作为输入，预测下一个单词，实现文本生成任务。\n",
    "\n",
    "\n",
    "![示例图片](pics/GRU.png)\n",
    "\n",
    "\n",
    "文本的预处理 训练数据生成与前面一致\n",
    "\n",
    "\n",
    "### GRU 模型构建\n",
    "\n",
    "实现了一个基于 GRU 的文本生成模型，通过输入文本序列预测下一个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14ee692edfce0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(GRUTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)  # (B, L, embed_dim)\n",
    "        output, hidden = self.gru(embedded, hidden)  # (B, L, hidden_dim)\n",
    "        output = self.fc(output[:, -1, :])  # 只取最后一个时间步的输出进行预测\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c265257cd9565af",
   "metadata": {},
   "source": [
    "定义模型所需参数、实例化模型、损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59bd931bf15b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = GRUTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f656509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 111/111 [00:09<00:00, 11.37it/s, loss=10.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 10.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 111/111 [00:09<00:00, 11.79it/s, loss=7.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss: 7.3869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 111/111 [00:09<00:00, 11.81it/s, loss=6.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss: 7.2394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 111/111 [00:09<00:00, 11.83it/s, loss=7.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss: 7.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 111/111 [00:09<00:00, 11.90it/s, loss=6.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss: 6.8987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 111/111 [00:09<00:00, 12.06it/s, loss=7.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss: 6.6383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 111/111 [00:09<00:00, 12.05it/s, loss=6.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss: 6.2195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 111/111 [00:08<00:00, 12.69it/s, loss=6.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss: 5.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 111/111 [00:08<00:00, 12.64it/s, loss=5.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss: 5.0456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 111/111 [00:08<00:00, 12.63it/s, loss=4.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss: 4.4338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 111/111 [00:08<00:00, 12.64it/s, loss=3.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss: 3.7887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 111/111 [00:08<00:00, 12.68it/s, loss=3.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss: 3.1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 111/111 [00:08<00:00, 12.71it/s, loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss: 2.5889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 111/111 [00:08<00:00, 12.72it/s, loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss: 2.0350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 111/111 [00:08<00:00, 12.74it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss: 1.5672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 111/111 [00:08<00:00, 12.67it/s, loss=0.765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss: 1.2053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 111/111 [00:08<00:00, 12.65it/s, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss: 0.9292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 111/111 [00:08<00:00, 12.64it/s, loss=0.907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss: 0.7046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 111/111 [00:08<00:00, 12.61it/s, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Avg Loss: 0.5109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 111/111 [00:08<00:00, 12.63it/s, loss=0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Avg Loss: 0.3499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        epoch_grad_norm = None\n",
    "\n",
    "        for X_batch, Y_batch in progress_bar:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad0ad957f31fa3",
   "metadata": {},
   "source": [
    "### GRU 模型训练\n",
    "\n",
    "GRU 训练过程也与LSTM保持一致\n",
    "\n",
    "\n",
    "### GRU 模型测试\n",
    "\n",
    "GRU 生成文本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c79300a967f2fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "🔹 模型生成的文本：\n",
      "\n",
      "the race is on: second private team sets launch date for human spaceflight (space.com) space.com - toronto, canada -- a second\\team of rocketeers competing for the #36;10 million ansari x prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket. in overall piracy.\"\\\\ aspect for the world are berlin, meaning up up that the dignified he's world need the world wikinews to homo deltas of need for a much smaller\\string one), the world and how 25 knuckle ...\\\\ one the dems before of a much pr to conception, the open's operating it was 21 ...\\\\ there the homo running, there to homo software. to berlin, most to homo troops? to homo statesman. physical to seen: the u.s. deltas of a precursor with one the posts to one), it http://www.lowing.org/fonts/ of the especially of the favorite there the especially aug homo\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text, num_words=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = tokenize(start_text)\n",
    "    input_seq = numericalize(start_text).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "\n",
    "        # 计算 softmax，并应用温度系数\n",
    "        logits = output.squeeze(0) / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # 采样新词\n",
    "        predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        next_word = vocab[predicted_id]\n",
    "        words.append(next_word)\n",
    "\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_id]], dtype=torch.long, device=device)],\n",
    "                              dim=1)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 生成文本\n",
    "print(\"\\nGenerated Text:\")\n",
    "test_text = dataset[\"test\"][1][\"text\"]\n",
    "# 取前 100 个单词作为前缀\n",
    "test_prefix = \" \".join(test_text.split()[:100])\n",
    "\n",
    "# 让模型基于该前缀生成 100 个词\n",
    "generated_text = generate_text(model, test_prefix, 100, temperature=0.8)\n",
    "print(\"\\n🔹 模型生成的文本：\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4dc9495339214",
   "metadata": {},
   "source": [
    "借助RNN文本生成任务中计算困惑的函数，计算一下GRU在generated_text上的困惑度。\n",
    "\n",
    "\n",
    "    思考题7：这三个困惑度可以直接比较吗？分析一下。\n",
    "\n",
    "    思考题8：GRU 只有两个门（更新门和重置门），相比 LSTM 少了一个门控单元，这样的设计有什么优缺点？\n",
    "\n",
    "    思考题9：在低算力设备（如手机）上，RNN、LSTM 和 GRU 哪个更适合部署？为什么？\n",
    "\n",
    "    思考题10：如果就是要使用RNN模型，原先的代码还有哪里可以优化的地方？请给出修改部分以及实验结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83b4dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU Perplexity (PPL): 4311.2959\n"
     ]
    }
   ],
   "source": [
    "# 计算 GRU 在生成文本上的困惑度\n",
    "ppl_gru = compute_perplexity(model, generated_text, vocab_dict)\n",
    "print(f\"GRU Perplexity (PPL): {ppl_gru:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8770a",
   "metadata": {},
   "source": [
    "思考题10： 优化的 RNN 代码：（对训练过程优化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef35147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 111/111 [00:09<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 9.8120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 111/111 [00:08<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss: 7.7030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 111/111 [00:09<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss: 7.7327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 111/111 [00:09<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss: 7.6739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 111/111 [00:08<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss: 7.7496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 111/111 [00:09<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss: 7.7442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 111/111 [00:08<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss: 7.6537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 111/111 [00:08<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss: 7.6673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 111/111 [00:08<00:00, 13.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss: 7.7670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 111/111 [00:09<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss: 7.3499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 111/111 [00:09<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss: 6.7275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 111/111 [00:09<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss: 6.6130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 111/111 [00:08<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss: 6.5615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 111/111 [00:07<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss: 6.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 111/111 [00:09<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss: 6.4618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 111/111 [00:09<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss: 6.2355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 111/111 [00:09<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss: 6.1678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 111/111 [00:07<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss: 6.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 111/111 [00:09<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Avg Loss: 6.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 111/111 [00:09<00:00, 12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Avg Loss: 5.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class RNNTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super(RNNTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)#将输入的单词索引转换为 embed_dim 维的向量。\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)#构建一个 RNN 层，用于处理序列数据。\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)#将 RNN 隐藏状态 映射到 词汇表大小的向量，用于预测下一个单词的概率分布。\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        #输入 x 形状：(batch_size, seq_len)\n",
    "        #输出 embedded 形状：(batch_size, seq_len, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        #输入 embedded 形状：(batch_size, seq_len, embed_dim)\n",
    "        #输出 output 形状：(batch_size, seq_len, hidden_dim)（所有时间步的隐藏状态）\n",
    "        #输出 hidden 形状：(num_layers, batch_size, hidden_dim)（最后一个时间步的隐藏状态）\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        #只取 最后一个时间步的隐藏状态 output[:, -1, :] 作为输入\n",
    "        #通过全连接层 self.fc 将隐藏状态转换为词汇表大小的分布（用于预测下一个单词）\n",
    "        #最终 output 形状：(batch_size, vocab_size)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "        \n",
    "embed_dim = 128\n",
    "hidden_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = RNNTextGenerator(vocab_size, embed_dim, hidden_dim, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, epochs=5, clip=5.0):\n",
    "    model.train()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # 学习率衰减\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, Y_batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # 梯度裁剪\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()  # 更新学习率\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader, epochs=20, clip=5.0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
