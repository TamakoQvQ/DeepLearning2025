{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0ecc530-9808-451a-b6f7-503f21ecb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a27403b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28370de4-463b-4312-9c38-f87c0bf5c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOSIDataset(Dataset):\n",
    "    def __init__(self, data_path, split='train'):\n",
    "        # Load the data from the pickle file\n",
    "        with open(data_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Select the appropriate split\n",
    "        self.vision = data[split]['vision']\n",
    "        self.text = data[split]['text']\n",
    "        self.audio = data[split]['audio']\n",
    "        self.labels = data[split]['labels']\n",
    "        self.ids = data[split]['id']\n",
    "\n",
    "        # audio数据中存在坏点需要处理：\n",
    "        self.audio[self.audio == float('inf')] = 0.0\n",
    "        self.audio[self.audio == float('-inf')] = 0.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the features and label for the given index\n",
    "        vision = torch.tensor(self.vision[idx], dtype=torch.float32)\n",
    "        text = torch.tensor(self.text[idx], dtype=torch.float32)\n",
    "        audio = torch.tensor(self.audio[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32).squeeze()\n",
    "\n",
    "        return vision, text, audio, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a4a98-63c0-45b3-bd91-d0a6ad3e1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class MultimodalSentimentAnalysisTransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 128\n",
    "        self.num_layers = 1\n",
    "        self.num_heads = 4\n",
    "\n",
    "        self.vision_norm = nn.LayerNorm(35)\n",
    "        self.text_norm = nn.LayerNorm(300)\n",
    "        self.audio_norm = nn.LayerNorm(74)\n",
    "\n",
    "        self.vision_fc = nn.Linear(35, self.hidden_dim)\n",
    "        self.text_fc = nn.Linear(300, self.hidden_dim)\n",
    "        self.audio_fc = nn.Linear(74, self.hidden_dim)\n",
    "\n",
    "        self.pe = PositionalEncoding(self.hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_dim, nhead=self.num_heads, dim_feedforward=256, dropout=0.1, batch_first=True)\n",
    "        self.vision_transformer = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "        self.text_transformer = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "        self.audio_transformer = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "        self.mm_transformer = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vision, text, audio):\n",
    "        vision = self.vision_norm(vision)\n",
    "        text = self.text_norm(text)\n",
    "        audio = self.audio_norm(audio)\n",
    "\n",
    "        vision = self.pe(F.relu(self.vision_fc(vision)))\n",
    "        text = self.pe(F.relu(self.text_fc(text)))\n",
    "        audio = self.pe(F.relu(self.audio_fc(audio)))\n",
    "\n",
    "        vision_feat = self.vision_transformer(vision).mean(dim=1)\n",
    "        text_feat = self.text_transformer(text).mean(dim=1)\n",
    "        audio_feat = self.audio_transformer(audio).mean(dim=1)\n",
    "\n",
    "        fused = vision_feat + text_feat + audio_feat\n",
    "        fused = fused.unsqueeze(1)\n",
    "\n",
    "        fusion_tensor = self.mm_transformer(fused).squeeze(1)\n",
    "\n",
    "        output = self.fc(fusion_tensor)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output * 6 - 3\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8554f28a-331c-424b-a15f-856484527278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mosi_regression(y_pred, y_true, exclude_zero=False):\n",
    "    test_preds = y_pred.view(-1).cpu().detach().numpy()\n",
    "    test_truth = y_true.view(-1).cpu().detach().numpy()\n",
    "\n",
    "    test_preds_a7 = np.clip(test_preds, a_min=-3., a_max=3.)\n",
    "    test_truth_a7 = np.clip(test_truth, a_min=-3., a_max=3.)\n",
    "    test_preds_a5 = np.clip(test_preds, a_min=-2., a_max=2.)\n",
    "    test_truth_a5 = np.clip(test_truth, a_min=-2., a_max=2.)\n",
    "\n",
    "    mae = np.mean(np.absolute(test_preds - test_truth))\n",
    "    corr = np.corrcoef(test_preds, test_truth)[0][1]\n",
    "    mult_a7 = multiclass_acc(test_preds_a7, test_truth_a7)\n",
    "    mult_a5 = multiclass_acc(test_preds_a5, test_truth_a5)\n",
    "    \n",
    "    non_zeros = np.array([i for i, e in enumerate(test_truth) if e != 0])\n",
    "    non_zeros_binary_truth = (test_truth[non_zeros] > 0)\n",
    "    non_zeros_binary_preds = (test_preds[non_zeros] > 0)\n",
    "\n",
    "    non_zeros_acc2 = accuracy_score(non_zeros_binary_preds, non_zeros_binary_truth)\n",
    "    non_zeros_f1_score = f1_score(non_zeros_binary_preds, non_zeros_binary_truth, average='weighted')\n",
    "    \n",
    "    eval_results = {\n",
    "        \"Non0_acc_2\":  round(non_zeros_acc2, 4),\n",
    "        \"Non0_F1_score\": round(non_zeros_f1_score, 4),\n",
    "        \"Mult_acc_5\": round(mult_a5, 4),\n",
    "        \"Mult_acc_7\": round(mult_a7, 4),\n",
    "        \"MAE\": round(mae, 4),\n",
    "        \"Corr\": round(corr, 4)\n",
    "    }\n",
    "    return eval_results\n",
    "\n",
    "def multiclass_acc(y_pred, y_true):\n",
    "    y_pred = np.round(y_pred)\n",
    "    y_true = np.round(y_true)\n",
    "\n",
    "    acc = (y_pred == y_true).sum() / len(y_true)\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20bce85d-d302-4029-abe3-74c3fdaeee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, device, epochs):\n",
    "    model.to(device)\n",
    "\n",
    "    best_corr = 0.\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (vision, text, audio, labels) in enumerate(train_loader):\n",
    "            vision, text, audio, labels = vision.to(device), text.to(device), audio.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(vision, text, audio)\n",
    "            loss = criterion(outputs.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "        val_corr = validate_model(model, valid_loader, criterion, device)\n",
    "\n",
    "        if val_corr > best_corr:\n",
    "            best_corr = val_corr\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f\"Best model saved with val_corr {best_corr} at epoch {best_epoch}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d0cbfa4-2c90-4d6a-bc5c-76285e8ef036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for vision, text, audio, labels in valid_loader:\n",
    "            vision, text, audio, labels = vision.to(device), text.to(device), audio.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(vision, text, audio)\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    print(f'Validation Loss: {valid_loss/len(valid_loader):.4f}')\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # add 计算评价指标\n",
    "    eval_results = eval_mosi_regression(all_preds, all_labels)\n",
    "    print(eval_results)\n",
    "\n",
    "    return eval_results[\"Corr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52dfb72b-e874-45b8-b745-191147d63b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # 固定随机数种子，确保实验结果可重复性\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    \n",
    "    # add 定义损失函数criterion, 使用均方误差损失。可以使用pytorch封装好的函数，也可以根据公式手写：\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    learning_rate = 1e-3\n",
    "    epochs = 20\n",
    "    \n",
    "    # add: Initialize the model.\n",
    "    model = model = MultimodalSentimentAnalysisTransformerModel()\n",
    "\n",
    "    data_path = './mosi_raw.pkl'\n",
    "    # add 初始化训练集和验证集的数据集类\n",
    "    train_dataset = MOSIDataset(data_path, split='train')\n",
    "    valid_dataset = MOSIDataset(data_path, split='valid')\n",
    "    # add 初始化训练集和验证集的加载器，要求batch_size=16\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Initialize the optimizer and scheduler.\n",
    "    # add: 使用Adam优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs*len(train_loader))\n",
    "\n",
    "    # add 调用训练函数，注意传入对应参数：\n",
    "    train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, device, epochs)\n",
    "\n",
    "    # 加载最佳epoch参数\n",
    "    best_model_state = torch.load('best_model.pth')\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # 初始化测试集的数据集类和加载器\n",
    "    test_dataset = MOSIDataset(data_path, split='test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    print(\"\\n========== test results: ==========\\n\")\n",
    "    validate_model(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ad2721-c1c0-406f-9958-ccd17805f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch [1/20], Loss: 2.5707\n",
      "Validation Loss: 2.5931\n",
      "{'Non0_acc_2': 0.607, 'Non0_F1_score': 0.7271, 'Mult_acc_5': np.float64(0.229), 'Mult_acc_7': np.float64(0.229), 'MAE': np.float32(1.391), 'Corr': np.float64(0.0504)}\n",
      "Epoch [2/20], Loss: 1.7753\n",
      "Validation Loss: 1.5293\n",
      "{'Non0_acc_2': 0.7413, 'Non0_F1_score': 0.7383, 'Mult_acc_5': np.float64(0.3131), 'Mult_acc_7': np.float64(0.285), 'MAE': np.float32(1.0338), 'Corr': np.float64(0.636)}\n",
      "Epoch [3/20], Loss: 1.2868\n",
      "Validation Loss: 1.3809\n",
      "{'Non0_acc_2': 0.806, 'Non0_F1_score': 0.8038, 'Mult_acc_5': np.float64(0.4112), 'Mult_acc_7': np.float64(0.3645), 'MAE': np.float32(0.9498), 'Corr': np.float64(0.6897)}\n",
      "Epoch [4/20], Loss: 1.1111\n",
      "Validation Loss: 1.2706\n",
      "{'Non0_acc_2': 0.8358, 'Non0_F1_score': 0.8356, 'Mult_acc_5': np.float64(0.4019), 'Mult_acc_7': np.float64(0.3411), 'MAE': np.float32(0.9208), 'Corr': np.float64(0.709)}\n",
      "Epoch [5/20], Loss: 0.9892\n",
      "Validation Loss: 1.2126\n",
      "{'Non0_acc_2': 0.8259, 'Non0_F1_score': 0.8281, 'Mult_acc_5': np.float64(0.4112), 'Mult_acc_7': np.float64(0.3224), 'MAE': np.float32(0.8778), 'Corr': np.float64(0.7201)}\n",
      "Epoch [6/20], Loss: 0.8646\n",
      "Validation Loss: 1.3370\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8138, 'Mult_acc_5': np.float64(0.4486), 'Mult_acc_7': np.float64(0.3645), 'MAE': np.float32(0.9055), 'Corr': np.float64(0.7204)}\n",
      "Epoch [7/20], Loss: 0.7605\n",
      "Validation Loss: 1.2712\n",
      "{'Non0_acc_2': 0.8109, 'Non0_F1_score': 0.8137, 'Mult_acc_5': np.float64(0.3925), 'Mult_acc_7': np.float64(0.3224), 'MAE': np.float32(0.9123), 'Corr': np.float64(0.7034)}\n",
      "Epoch [8/20], Loss: 0.7000\n",
      "Validation Loss: 1.2476\n",
      "{'Non0_acc_2': 0.806, 'Non0_F1_score': 0.8044, 'Mult_acc_5': np.float64(0.4065), 'Mult_acc_7': np.float64(0.3411), 'MAE': np.float32(0.9019), 'Corr': np.float64(0.7189)}\n",
      "Epoch [9/20], Loss: 0.6152\n",
      "Validation Loss: 1.2690\n",
      "{'Non0_acc_2': 0.796, 'Non0_F1_score': 0.8029, 'Mult_acc_5': np.float64(0.3832), 'Mult_acc_7': np.float64(0.3084), 'MAE': np.float32(0.9072), 'Corr': np.float64(0.7096)}\n",
      "Epoch [10/20], Loss: 0.5112\n",
      "Validation Loss: 1.4578\n",
      "{'Non0_acc_2': 0.8109, 'Non0_F1_score': 0.8093, 'Mult_acc_5': np.float64(0.4346), 'Mult_acc_7': np.float64(0.3411), 'MAE': np.float32(0.936), 'Corr': np.float64(0.6826)}\n",
      "Epoch [11/20], Loss: 0.4756\n",
      "Validation Loss: 1.3304\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8204, 'Mult_acc_5': np.float64(0.3879), 'Mult_acc_7': np.float64(0.3131), 'MAE': np.float32(0.9096), 'Corr': np.float64(0.687)}\n",
      "Epoch [12/20], Loss: 0.4052\n",
      "Validation Loss: 1.4272\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8144, 'Mult_acc_5': np.float64(0.4346), 'Mult_acc_7': np.float64(0.3505), 'MAE': np.float32(0.9295), 'Corr': np.float64(0.6811)}\n",
      "Epoch [13/20], Loss: 0.3681\n",
      "Validation Loss: 1.4810\n",
      "{'Non0_acc_2': 0.801, 'Non0_F1_score': 0.7995, 'Mult_acc_5': np.float64(0.4019), 'Mult_acc_7': np.float64(0.3411), 'MAE': np.float32(0.9564), 'Corr': np.float64(0.6556)}\n",
      "Epoch [14/20], Loss: 0.3071\n",
      "Validation Loss: 1.5765\n",
      "{'Non0_acc_2': 0.801, 'Non0_F1_score': 0.7992, 'Mult_acc_5': np.float64(0.4299), 'Mult_acc_7': np.float64(0.3551), 'MAE': np.float32(0.9711), 'Corr': np.float64(0.664)}\n",
      "Epoch [15/20], Loss: 0.2713\n",
      "Validation Loss: 1.5889\n",
      "{'Non0_acc_2': 0.801, 'Non0_F1_score': 0.7992, 'Mult_acc_5': np.float64(0.4299), 'Mult_acc_7': np.float64(0.3551), 'MAE': np.float32(0.9801), 'Corr': np.float64(0.656)}\n",
      "Epoch [16/20], Loss: 0.2428\n",
      "Validation Loss: 1.6284\n",
      "{'Non0_acc_2': 0.806, 'Non0_F1_score': 0.805, 'Mult_acc_5': np.float64(0.4393), 'Mult_acc_7': np.float64(0.3598), 'MAE': np.float32(0.9855), 'Corr': np.float64(0.6485)}\n",
      "Epoch [17/20], Loss: 0.2229\n",
      "Validation Loss: 1.6663\n",
      "{'Non0_acc_2': 0.806, 'Non0_F1_score': 0.8044, 'Mult_acc_5': np.float64(0.4299), 'Mult_acc_7': np.float64(0.3505), 'MAE': np.float32(0.9994), 'Corr': np.float64(0.6413)}\n",
      "Epoch [18/20], Loss: 0.2094\n",
      "Validation Loss: 1.6654\n",
      "{'Non0_acc_2': 0.8109, 'Non0_F1_score': 0.8095, 'Mult_acc_5': np.float64(0.4299), 'Mult_acc_7': np.float64(0.3505), 'MAE': np.float32(0.9983), 'Corr': np.float64(0.6428)}\n",
      "Epoch [19/20], Loss: 0.2070\n",
      "Validation Loss: 1.6504\n",
      "{'Non0_acc_2': 0.8109, 'Non0_F1_score': 0.8098, 'Mult_acc_5': np.float64(0.4393), 'Mult_acc_7': np.float64(0.3598), 'MAE': np.float32(0.9926), 'Corr': np.float64(0.6435)}\n",
      "Epoch [20/20], Loss: 0.2024\n",
      "Validation Loss: 1.6594\n",
      "{'Non0_acc_2': 0.8109, 'Non0_F1_score': 0.8098, 'Mult_acc_5': np.float64(0.4252), 'Mult_acc_7': np.float64(0.3458), 'MAE': np.float32(0.9961), 'Corr': np.float64(0.6427)}\n",
      "Best model saved with val_corr 0.7204 at epoch 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_173447/1790762078.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_state = torch.load('best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== test results: ==========\n",
      "\n",
      "Validation Loss: 1.9641\n",
      "{'Non0_acc_2': 0.7439, 'Non0_F1_score': 0.7439, 'Mult_acc_5': np.float64(0.3848), 'Mult_acc_7': np.float64(0.3353), 'MAE': np.float32(1.0541), 'Corr': np.float64(0.5593)}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
