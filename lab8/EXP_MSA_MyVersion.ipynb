{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ecc530-9808-451a-b6f7-503f21ecb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27403b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28370de4-463b-4312-9c38-f87c0bf5c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOSIDataset(Dataset):\n",
    "    def __init__(self, data_path, split='train'):\n",
    "        # Load the data from the pickle file\n",
    "        with open(data_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Select the appropriate split\n",
    "        self.vision = data[split]['vision']\n",
    "        self.text = data[split]['text']\n",
    "        self.audio = data[split]['audio']\n",
    "        self.labels = data[split]['labels']\n",
    "        self.ids = data[split]['id']\n",
    "\n",
    "        # audio数据中存在坏点需要处理：\n",
    "        self.audio[self.audio == float('inf')] = 0.0\n",
    "        self.audio[self.audio == float('-inf')] = 0.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the features and label for the given index\n",
    "        vision = torch.tensor(self.vision[idx], dtype=torch.float32)\n",
    "        text = torch.tensor(self.text[idx], dtype=torch.float32)\n",
    "        audio = torch.tensor(self.audio[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32).squeeze()\n",
    "\n",
    "        return vision, text, audio, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812a4a98-63c0-45b3-bd91-d0a6ad3e1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=256, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.project = nn.Linear(input_dim, d_model)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = F.relu(self.project(x))\n",
    "        x = self.encoder(x)\n",
    "        return x  # (batch, seq, d_model)\n",
    "\n",
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, d_model=128):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, d_model))  # Learnable query vector\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads=4, batch_first=True)\n",
    "\n",
    "    def forward(self, v, t, a):\n",
    "        # Concatenate: (B, 3, D)\n",
    "        x = torch.stack([v[:, -1], t[:, -1], a[:, -1]], dim=1)\n",
    "        q = self.query.expand(x.size(0), -1, -1)  # (B, 1, D)\n",
    "        fused, _ = self.attn(q, x, x)\n",
    "        return fused.squeeze(1)\n",
    "\n",
    "class MultimodalSentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vision_encoder = TransformerEncoderBlock(35)\n",
    "        self.text_encoder = TransformerEncoderBlock(300)\n",
    "        self.audio_encoder = TransformerEncoderBlock(74)\n",
    "\n",
    "        self.fusion = AttentionFusion(d_model=128)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, vision, text, audio):\n",
    "        vision_feat = self.vision_encoder(vision)\n",
    "        text_feat = self.text_encoder(text)\n",
    "        audio_feat = self.audio_encoder(audio)\n",
    "\n",
    "        fused = self.fusion(vision_feat, text_feat, audio_feat)\n",
    "\n",
    "        out = torch.sigmoid(self.fc(fused)) * 6 - 3\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8554f28a-331c-424b-a15f-856484527278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mosi_regression(y_pred, y_true, exclude_zero=False):\n",
    "    test_preds = y_pred.view(-1).cpu().detach().numpy()\n",
    "    test_truth = y_true.view(-1).cpu().detach().numpy()\n",
    "\n",
    "    test_preds_a7 = np.clip(test_preds, a_min=-3., a_max=3.)\n",
    "    test_truth_a7 = np.clip(test_truth, a_min=-3., a_max=3.)\n",
    "    test_preds_a5 = np.clip(test_preds, a_min=-2., a_max=2.)\n",
    "    test_truth_a5 = np.clip(test_truth, a_min=-2., a_max=2.)\n",
    "\n",
    "    mae = np.mean(np.absolute(test_preds - test_truth))\n",
    "    corr = np.corrcoef(test_preds, test_truth)[0][1]\n",
    "    mult_a7 = multiclass_acc(test_preds_a7, test_truth_a7)\n",
    "    mult_a5 = multiclass_acc(test_preds_a5, test_truth_a5)\n",
    "    \n",
    "    non_zeros = np.array([i for i, e in enumerate(test_truth) if e != 0])\n",
    "    non_zeros_binary_truth = (test_truth[non_zeros] > 0)\n",
    "    non_zeros_binary_preds = (test_preds[non_zeros] > 0)\n",
    "\n",
    "    non_zeros_acc2 = accuracy_score(non_zeros_binary_preds, non_zeros_binary_truth)\n",
    "    non_zeros_f1_score = f1_score(non_zeros_binary_preds, non_zeros_binary_truth, average='weighted')\n",
    "    \n",
    "    eval_results = {\n",
    "        \"Non0_acc_2\":  round(non_zeros_acc2, 4),\n",
    "        \"Non0_F1_score\": round(non_zeros_f1_score, 4),\n",
    "        \"Mult_acc_5\": round(mult_a5, 4),\n",
    "        \"Mult_acc_7\": round(mult_a7, 4),\n",
    "        \"MAE\": round(mae, 4),\n",
    "        \"Corr\": round(corr, 4)\n",
    "    }\n",
    "    return eval_results\n",
    "\n",
    "def multiclass_acc(y_pred, y_true):\n",
    "    y_pred = np.round(y_pred)\n",
    "    y_true = np.round(y_true)\n",
    "\n",
    "    acc = (y_pred == y_true).sum() / len(y_true)\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20bce85d-d302-4029-abe3-74c3fdaeee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, device, epochs):\n",
    "    model.to(device)\n",
    "\n",
    "    best_corr = 0.\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (vision, text, audio, labels) in enumerate(train_loader):\n",
    "            vision, text, audio, labels = vision.to(device), text.to(device), audio.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(vision, text, audio)\n",
    "            loss = criterion(outputs.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "        val_corr = validate_model(model, valid_loader, criterion, device)\n",
    "\n",
    "        if val_corr > best_corr:\n",
    "            best_corr = val_corr\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f\"Best model saved with val_corr {best_corr} at epoch {best_epoch}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0cbfa4-2c90-4d6a-bc5c-76285e8ef036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for vision, text, audio, labels in valid_loader:\n",
    "            vision, text, audio, labels = vision.to(device), text.to(device), audio.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(vision, text, audio)\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    print(f'Validation Loss: {valid_loss/len(valid_loader):.4f}')\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    eval_results = eval_mosi_regression(all_preds, all_labels)\n",
    "    print(eval_results)\n",
    "\n",
    "    return eval_results[\"Corr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52dfb72b-e874-45b8-b745-191147d63b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    learning_rate = 1e-3\n",
    "    epochs = 20\n",
    "    \n",
    "    model = model = MultimodalSentimentAnalysisModel()\n",
    "\n",
    "    data_path = './mosi_raw.pkl'\n",
    "    # add 初始化训练集和验证集的数据集类\n",
    "    train_dataset = MOSIDataset(data_path, split='train')\n",
    "    valid_dataset = MOSIDataset(data_path, split='valid')\n",
    "    # add 初始化训练集和验证集的加载器，要求batch_size=16\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Initialize the optimizer and scheduler.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs*len(train_loader))\n",
    "\n",
    "    train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, device, epochs)\n",
    "\n",
    "    best_model_state = torch.load('best_model.pth')\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    test_dataset = MOSIDataset(data_path, split='test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    print(\"\\n========== test results: ==========\\n\")\n",
    "    validate_model(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3ad2721-c1c0-406f-9958-ccd17805f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch [1/20], Loss: 0.8023\n",
      "Validation Loss: 0.7525\n",
      "{'Non0_acc_2': 0.7313, 'Non0_F1_score': 0.7353, 'Mult_acc_5': np.float64(0.2336), 'Mult_acc_7': np.float64(0.2196), 'MAE': np.float32(1.1882), 'Corr': np.float64(0.5174)}\n",
      "Epoch [2/20], Loss: 0.5454\n",
      "Validation Loss: 0.5959\n",
      "{'Non0_acc_2': 0.7711, 'Non0_F1_score': 0.7845, 'Mult_acc_5': np.float64(0.3551), 'Mult_acc_7': np.float64(0.3084), 'MAE': np.float32(1.0081), 'Corr': np.float64(0.6519)}\n",
      "Epoch [3/20], Loss: 0.4668\n",
      "Validation Loss: 0.4821\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8177, 'Mult_acc_5': np.float64(0.4112), 'Mult_acc_7': np.float64(0.3271), 'MAE': np.float32(0.8829), 'Corr': np.float64(0.7279)}\n",
      "Epoch [4/20], Loss: 0.3585\n",
      "Validation Loss: 0.4634\n",
      "{'Non0_acc_2': 0.8308, 'Non0_F1_score': 0.834, 'Mult_acc_5': np.float64(0.4112), 'Mult_acc_7': np.float64(0.3318), 'MAE': np.float32(0.8624), 'Corr': np.float64(0.7579)}\n",
      "Epoch [5/20], Loss: 0.3084\n",
      "Validation Loss: 0.5424\n",
      "{'Non0_acc_2': 0.7662, 'Non0_F1_score': 0.7637, 'Mult_acc_5': np.float64(0.3832), 'Mult_acc_7': np.float64(0.3271), 'MAE': np.float32(0.9551), 'Corr': np.float64(0.7077)}\n",
      "Epoch [6/20], Loss: 0.2814\n",
      "Validation Loss: 0.4947\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8183, 'Mult_acc_5': np.float64(0.4346), 'Mult_acc_7': np.float64(0.3645), 'MAE': np.float32(0.8951), 'Corr': np.float64(0.713)}\n",
      "Epoch [7/20], Loss: 0.2328\n",
      "Validation Loss: 0.4703\n",
      "{'Non0_acc_2': 0.8308, 'Non0_F1_score': 0.8313, 'Mult_acc_5': np.float64(0.4626), 'Mult_acc_7': np.float64(0.3598), 'MAE': np.float32(0.8596), 'Corr': np.float64(0.7367)}\n",
      "Epoch [8/20], Loss: 0.1886\n",
      "Validation Loss: 0.4659\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8171, 'Mult_acc_5': np.float64(0.4673), 'Mult_acc_7': np.float64(0.3879), 'MAE': np.float32(0.8556), 'Corr': np.float64(0.7282)}\n",
      "Epoch [9/20], Loss: 0.1713\n",
      "Validation Loss: 0.5362\n",
      "{'Non0_acc_2': 0.806, 'Non0_F1_score': 0.8044, 'Mult_acc_5': np.float64(0.3925), 'Mult_acc_7': np.float64(0.3037), 'MAE': np.float32(0.9424), 'Corr': np.float64(0.7094)}\n",
      "Epoch [10/20], Loss: 0.1303\n",
      "Validation Loss: 0.4690\n",
      "{'Non0_acc_2': 0.8358, 'Non0_F1_score': 0.8364, 'Mult_acc_5': np.float64(0.4579), 'Mult_acc_7': np.float64(0.3598), 'MAE': np.float32(0.8482), 'Corr': np.float64(0.7299)}\n",
      "Epoch [11/20], Loss: 0.1033\n",
      "Validation Loss: 0.5000\n",
      "{'Non0_acc_2': 0.806, 'Non0_F1_score': 0.8057, 'Mult_acc_5': np.float64(0.3972), 'Mult_acc_7': np.float64(0.3131), 'MAE': np.float32(0.8967), 'Corr': np.float64(0.7207)}\n",
      "Epoch [12/20], Loss: 0.0928\n",
      "Validation Loss: 0.5151\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8157, 'Mult_acc_5': np.float64(0.3972), 'Mult_acc_7': np.float64(0.3084), 'MAE': np.float32(0.911), 'Corr': np.float64(0.7114)}\n",
      "Epoch [13/20], Loss: 0.0748\n",
      "Validation Loss: 0.4970\n",
      "{'Non0_acc_2': 0.8209, 'Non0_F1_score': 0.8213, 'Mult_acc_5': np.float64(0.4252), 'Mult_acc_7': np.float64(0.3364), 'MAE': np.float32(0.8733), 'Corr': np.float64(0.7193)}\n",
      "Epoch [14/20], Loss: 0.0646\n",
      "Validation Loss: 0.4981\n",
      "{'Non0_acc_2': 0.8209, 'Non0_F1_score': 0.8213, 'Mult_acc_5': np.float64(0.4252), 'Mult_acc_7': np.float64(0.3411), 'MAE': np.float32(0.8909), 'Corr': np.float64(0.7117)}\n",
      "Epoch [15/20], Loss: 0.0580\n",
      "Validation Loss: 0.5109\n",
      "{'Non0_acc_2': 0.8109, 'Non0_F1_score': 0.8105, 'Mult_acc_5': np.float64(0.3832), 'Mult_acc_7': np.float64(0.2991), 'MAE': np.float32(0.9005), 'Corr': np.float64(0.7145)}\n",
      "Epoch [16/20], Loss: 0.0464\n",
      "Validation Loss: 0.5011\n",
      "{'Non0_acc_2': 0.8209, 'Non0_F1_score': 0.8209, 'Mult_acc_5': np.float64(0.4112), 'Mult_acc_7': np.float64(0.3271), 'MAE': np.float32(0.8865), 'Corr': np.float64(0.7189)}\n",
      "Epoch [17/20], Loss: 0.0438\n",
      "Validation Loss: 0.4979\n",
      "{'Non0_acc_2': 0.8259, 'Non0_F1_score': 0.8261, 'Mult_acc_5': np.float64(0.4206), 'Mult_acc_7': np.float64(0.3364), 'MAE': np.float32(0.8805), 'Corr': np.float64(0.7172)}\n",
      "Epoch [18/20], Loss: 0.0429\n",
      "Validation Loss: 0.5049\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8157, 'Mult_acc_5': np.float64(0.4065), 'Mult_acc_7': np.float64(0.3224), 'MAE': np.float32(0.8885), 'Corr': np.float64(0.7177)}\n",
      "Epoch [19/20], Loss: 0.0410\n",
      "Validation Loss: 0.5021\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8157, 'Mult_acc_5': np.float64(0.4065), 'Mult_acc_7': np.float64(0.3224), 'MAE': np.float32(0.8855), 'Corr': np.float64(0.7175)}\n",
      "Epoch [20/20], Loss: 0.0393\n",
      "Validation Loss: 0.5020\n",
      "{'Non0_acc_2': 0.8159, 'Non0_F1_score': 0.8157, 'Mult_acc_5': np.float64(0.4159), 'Mult_acc_7': np.float64(0.3318), 'MAE': np.float32(0.8851), 'Corr': np.float64(0.7174)}\n",
      "Best model saved with val_corr 0.7579 at epoch 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_242192/2386164866.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_state = torch.load('best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== test results: ==========\n",
      "\n",
      "Validation Loss: 0.7478\n",
      "{'Non0_acc_2': 0.6616, 'Non0_F1_score': 0.6614, 'Mult_acc_5': np.float64(0.293), 'Mult_acc_7': np.float64(0.2741), 'MAE': np.float32(1.1563), 'Corr': np.float64(0.5464)}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
